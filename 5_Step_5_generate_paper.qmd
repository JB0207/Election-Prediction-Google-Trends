---
title: "Can we predict multi-party elections with Google Trends data? Evidence across elections, data windows and model classes"
abstract-title: "Abstract"
abstract: "Google trends (GT), a service aggregating search queries on Google, has been used to predict various outcomes such as as the spread of influenza, automobile sales, unemployment claims, and travel destination planning [@choi_predicting_2012; @ginsberg_detecting_2009]. Political scientists used GT to predict elections and referendums across different countries and time periods, sometimes with more, sometimes with less success. We provide unique evidence on the predictive power of GT in the German multi-party systems forecasting four elections (2009, 2013, 2017, 2021) and make several contributions to this literature: First, we present one of the first attempts to predict a multi-party election using GT and highlight the specific challenges that originate from this setting. Second, we develop a framework that allows for fine-grained variation of the GT data window both in terms of window'width and distance to the event. In our case, we estimate 24000 models using GT data. Third, … Finally, we summarize the various challenges that researchers face when using Google trends for prediction that have often been ignored in previous research."
author:
  - name: Jan Behnert
  - name: Dean Lajic
  - name: Paul C. Bauer
date: 'last-modified'
format:
  pdf:
    toc: true
    fig-pos: 'h'
    includes:
      in_header: 'gt_packages.sty'
    fig-width: 7
    fig-height: 5
    number-sections: true
    include-in-header: 
      text: |
        \usepackage{booktabs}
        \usepackage{siunitx}
        \usepackage{multirow}
        \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{A\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{A\arabic{figure}}}
execute: 
  echo: false
  warning: false
  message: false
  cache: true
tbl-cap-location: top
fig-cap-location: top
editor:
  mode: source
bibliography: references.bib
csl: vancouver.csl
keywords: "Google Trends, Election, Prediction"
---

```{r}
# format:
#   docx:
#     reference-doc: custom-reference-doc.docx
```

```{r packages}
# Load packages ####
library(pacman)
p_load(gtrendsR,
       ggplot2,
       tidyverse,
       rvest,
       xml2,
       data.table,
       patchwork,
       lubridate,
       #ajfhelpR,
       jsonlite,
       kableExtra,
       gt,
       grid,
       ggh4x,
       htmlTable,
       forcats,
       modelsummary,
       stargazer)

```



```{r data-import}
# load(file  = "./Environments/Env_Merged_syntax_NEU 2023-01-22_09-06-42.RData")

# Delete unneccessary objects
# rm(list=setdiff(ls(), c("data_models", "data_predictions_final", "data_predictions"))) 


  data_models <- read_csv("data_models.csv")  %>%
    mutate(model_time_interval = duration(model_time_interval))
  data_predictions_final_mean <- read_csv("data_predictions_final_mean.csv") %>%
    mutate(model_time_interval = duration(model_time_interval)) %>%
  mutate(model_time_distance = election_date - GT_end_date)

```


# Introduction

Google trends (GT), a service aggregating search queries on Google, has been used to predict various outcomes such as the spread of influenza, automobile sales, unemployment claims, and travel destination planning [@choi_predicting_2012, @ginsberg_detecting_2009]. These sparked the interest of political scientists who subsequently used GT data to predict elections with binary outcomes, e.g., presidential elections in the US or referenda such as the Brexit referendum, and, often claim to be successful [@askitas_calling_2015, @askitas_predicting_2015, @mavragani_yes_2016, @mavragani_predicting_2019, @prado-roman_google_2021]. In principle, Google Trends could provide a cheap data source that may extend previous predictive models that rely on polling data as well as structural data such as previous election results [@gschwend_zweitstimme_2022, @munzert_zweitstimme_2017, @stoetzer_forecasting_2019]. Exploring new data sources such as GT is also warranted given the persisting discussion on the validity of polling data [@jennings_election_2018, @schnell_accuracy_2014, @wang_forecasting_2015].^[More recently, scholars have turned to using reweighted non-representative polls to predict elections [e.g., @wang_forecasting_2015].] 
In our study we pursue the following research question: Can we use Google Trends data to predict election results in a multi-party system? Thereby, we make a series of contributions to scholarship using GT for predictive purposes generally but also specifically for elections: First, we are among the first to evidence on the predictive power of GT in the multi-party system setting. Thereby, we use GT to predict four elections in Germany and highlight the specific challenges that originate from the multi-party context. We also provide a systematic review of previous research highlight variation across several important dimensions. This review both helps us to highlight our contributions and can represent a starting point for future research on GT predictions (see @tbl-2). Second, when using GT for predictions one of the most important choices lies in the GT data window on which those predictions are based. The data window may vary in terms of width, e.g., our prediction could be based on averaging 1 week of GT data as opposed to 3 weeks of GT data. And, the data window may vary in terms of distance to the event that shall be predicted, e.g., we could predict an election using a data window that ends just two days before the election or a data window that ends three months before the election. Granka (Granka 2013) suggested to explore moving averages to assess how well models withstand changes closer to the election date. While previous studies have varied these aspects, we are the first to build a framework that allows to cycle through fine-grained values of both width and distance testing the predictive accuracy of thousands of resulting models. Third, following previous studies, we compare the predictive power of different model classes. Besides, a model that only includes GT data, we explore the predictive accuracy of models that combine GT data with election data and polling data. Finally, we compare our predictions to simple polling data. This provides us with an answer as to whether GT really does represent an alternative to classic purely poll-based methods and whether combinations are fruitful. Finally, we provide a systematic overview of the challenges one faces in using GT data for predictions part of which have been neglected in prior research. These comprise choices on the GT platform, e.g., we can restrict that data to searches belonging to certain categories, but also the varying nature of GT data across samples. We discuss which previous studies have acknowledged these and other challenges and tackle them in a systematic fashion to study their impact. Thereby, we provide a blueprint for future GT research. Below, we start by providing an overview of research that leverages GT data for predictive purposes generally but also for political outcomes (see Section 2). In Section 3 we explain our methodological approach, the data we are collecting and using as well as the predictive models we are building. Section 4 presents our results namely the predictions of four elections. In the conclusion (Section 5) we summarize the most important insights.

# Using Google Trends to predict elections and other phenomena

The proportion of internet users aged 14 and over in Germany has risen from 37% in 1991 to 91% individuals aged 14 and older [@initiative_d21_share_2022]. In terms of search engines Google is by far the dominant search engine with market shares of 80.4% (desktop) and 96.8% (mobile) [@statcounter_desktop_2023]. Nonetheless, as we will discuss below, Google users are not necessarily representative of the German electorate.

## Predicting phenomena with Google Trends

Google has made its data on Google searches freely available for everyone in the year 2006 (see https://trends.google.com/). Rather than absolute numbers of searches, GT provides data on interest in a search term relative to all other search terms in a country or region over a selected period of time. GT data comes along with certain advantages such as cost-free access to aggregated big data, a sample that is (ideally) representative of all Google searches or an unﬁltered real-time sample. Soon after going public GT made headlines and the number of studies using GT data has grown significantly. In general, the assumption underlying those studies is that search queries in Google reﬂect the genuine interests or intentions of people. While GT was most frequently used in the ﬁeld of Computer Science, usage in other disciplines has picked up [@jun_ten_2018]. In a ground-breaking study Ginsberg and colleagues used GT data and its real-time nature to predict the spread of inﬂuenza, comparing its accuracy to predictions by the Government Agency Center for Disease Control and Prevention (CDC) [@ginsberg_detecting_2009]. This first study started a debate on the advantages but also deficiencies of GT flu predictions [@kandula_reappraising_2019, @lazer_parable_2014, @yang_accurate_2015]. The enormous potential of GT data was also demonstrated by Choi & Varian [@choi_predicting_2012], who predicted economic indicators including automobile sales, unemployment claims, travel destination planning, etc. The continued popularity is reﬂected in recently published papers that used GT data in the context of the COVID-19 pandemic. Brodeur et al. [@brodeur_covid-19_2021] for example examined whether COVID-19 and the associated lockdowns initiated in Europe and America led to changes in well-being.

## Predicting elections with Google Trends

@tbl-1 summarizes studies that have used GT data to predict elections (see also Table A1). Reviewing the literature a few aspects stand out. First, the focus usually lies on binary electoral outcomes. Prado-Román et al. [@prado-roman_google_2021], for example, were able to predict the ﬁnal results of all presidential elections in the US and Canada for the time period 2004 -- 2016. Polykalas et al. [@polykalas_general_2013] were able to forecast Greek and Spain election results. However, other studies found GT to be less helpful in predicting elections, despite their focus on binary settings [@harkan_predicting_2021, @wolf_trending_2018]. Other studies illustrated that GT data can be used to predict referendum results [@askitas_calling_2015, @askitas_predicting_2015). @mavragani_predicting_2019 successfully predicted six referendums in different countries in Europe between 2014 and 2016, for example the Brexit referendum. Polykalas et al. [@polykalas_algorithm_2013] used GT to predict elections in Germany, examining only the two most popular parties, the SPD and the CDU, trying to predict which of the two parties will win. In contrast to the previously mentioned studies, the authors additionally weight their GT predictions using previous election results to control for the selection bias, i.e., the fact that not everyone uses the internet. As indicated in @tbl-1, one of the few exceptions predicting multi-party elections is @sjovill_using_2020. Sjövill's thesis uses GT data to predict party shares focusing on the three Swedish federal elections 2010, 2014 and 2018. Sjövill [@sjovill_using_2020] emphasizes the importance of weighting the GT predictions using previous ﬁnal election results and polling data, to control for the sample selection bias of GT data. Sjövill [@sjovill_using_2020] compares diﬀerent models with the diﬀerent weighting methods and finds that they mostly have the same predictive accuracy as the average of pre-election opinion polls. The weighting method using actual polling data proved to be the most informative. Second, the GT data window that is used for the prediction is defined through its width, i.e., the end time minus the start time of the window, and the distance to the event that shall be predicted. Across studies there is strong variation in terms of the GT data windows chosen both in terms of width and distance (cf. @tbl-1). Most studies pick one to three different widths, usually one week or one month. And, in terms of distance studies usually pick a data window that ends just before the election. Naturally, the question arises whether the findings made across these studies would be the same if we were to vary the width and distance of the underlying GT data. It is one of our aims to provide a more systematic approach to choosing the data window comparing predictions for a wide variety of choices. Third, while researchers have compared GT predictions to classical polling data, they have also explored different weighting schemes that may decrease sample selection bias [@polykalas_algorithm_2013, @sjovill_using_2020). Sample selection bias is present if, e.g., voters of conservative parties are on average older and thus use the internet less. As a consequence they are underrepresented among Google users leading to an underestimation of the vote share of conservative parties. Polykalas et al. [@polykalas_algorithm_2013] weighted their GT predictions with election results from the previous election. Sjövill [@sjovill_using_2020] constructed three different models: A long-term model that weighs GT predictions with partys' previous election results; an intermediate model weighting the GT predictions using semi-annual and highly representative polling data from a respected election poll in Sweden; and a short-term model which used average monthly polling data for weighting. While all models came close to the results of the election polls in the election years studied, the short time model proved to be the most informative. Inspired by this research we also compare different weighting schemes as described in Section 3. Finally, with few exceptions, most studies remain silent on a set of important characteristics of the GT platform and its data. At the same time these characteristics can affect any predictive exercise. A first issue is that data provided by GT represents a random sample that changes over time [@raubenheimer_hey_2021]. As summarized in @tbl-1, column "Multiple GT datasets", we only found one study that bases its predictions on several GT data samples [@raubenheimer_hey_2021]. Following Raubenheimer et al. [@raubenheimer_hey_2021] we develop a systematic sampling strategy for GT data and average our predictions across those samples as described in Section 3.3 and Appendix "GT data collection and sampling error". A second issue is the selection of GT search terms. This selection potentially has the strongest effect on any predictions we make, hence, transparently communicating how and following which rational these terms have been selected is of utmost importance. A third issue is whether to further refine search terms by picking a category filter provided by Google. Such filters attempt to identify searches that belong to particular topics helping to identify only relevant searches. With the exception of Mavragani & Tsagarakis [@mavragani_predicting_2019] no previous studies made use of such categories (cf. column "Cat. used" in @tbl-1). Below, we compare the impact of basing predictions on searches refined through such a category and non-refined searches.

```{r}
#| label: tbl-1
#| tbl-cap: "Overview of choices in previous studies"
#| tbl-colwidths: auto
##
table_1 <- read_csv("Table_1.csv")
#Syntax
table_1[is.na(table_1)] = ""
kable(table_1, booktabs = TRUE) %>%
  kable_styling(font_size = 6)  %>%
      column_spec(1,width = "0.8in") %>%
      column_spec(2,width = "1in") %>%
      column_spec(3,width = "0.4in") %>%
      column_spec(4,width = "0.6in") %>%
      column_spec(5,width = "0.4in") %>%
      column_spec(6,width = "0.4in")  %>%
      column_spec(7,width = "0.3in")  %>%
      column_spec(8,width = "0.3in") %>%
      footnote("* = Non-binary outcome; ? = not enough information")
```




# Data and Method

## Google Trends as a data source

In our analysis we rely on three types of data, Google trends data (see Section 3.2-3.3) as well as data on from polls and actual election results (see Section 3.4). GT provides access to a largely unﬁltered sample of "real-time" searches on different topics (up to 36h before the actual time you conduct the search) or a ﬁltered and representative sample (as claimed by Google) of searches that are older than 36 hours starting from the year 2004. The data can be obtained for different search types that correspond to diﬀerent Google products like "Web search," "News," "Images," "Shopping" and "Youtube". Importantly, GT does not provide access to data on individual searches. Rather the data is anonymized and Google aggregates the data to the federal state level, country level or world level. Besides, it is possible to filter searches belonging to different categories, e.g., "Law and Government", with the aim of only getting searches for the word's meaning one is interested in. The result we get are a standardized, relative measure of search volume for a single word/search term, a combination of search terms using operators,^[Available Operators: No quotation marks (results for each word in your query), Quotation marks (coherent search phrase), Plus sign (serves as function of an OR-operator) and Minus sign (Excludes word after the operator) **(Google Trends Help 2023d)**.] or comparisons, i.e., one input in relation to the other inputs,^[Possible to compare up to ﬁve groups of terms at one time and up to 25 terms in each group (Google Trends Help 2023a).] over a selected time period. Google calculates how much search volume in each region a search term or query had, relative to all searches in that region. Using this information, Google assigns a measure of popularity to search terms (scale of 0 - 100), leaving out repeated searches from the same person over a short period of time and searches with apostrophes and other special characters [@google_trends_help_faq_2023]. The maximum of the scale corresponds to a search term's maximum level of popularity relative to other search terms and time periods.

## Search terms and category filter

The first step in using Google searches to predict phenomena is the selection of search terms on which we base our predictions. Since words may have multiple meanings, e.g., jaguar could be an animal or a car, GT provides a category filter to get data for the right version of the word. Only one previous study made use of the category filter in Google Trends [@mavragani_predicting_2019], a shortcoming since categories help purge the search term for multiple meanings and thus assure that one gets results for the right version of that search term. For instance, in our case adding the category "Law and Government" when searching for the search term "Grüne" assures that a political party is meant and not the color [@google_trends_help_refine_2023]. Below we describe how we selected the search terms and which selections we chose for the GT's category filter.

### Category filter

The first step in our analysis was to compare all supercategories within Google's "Web Search" product across our four elections (09', 13', 17' and 21'), to select the ideal category for our analysis.^[Supercategories: All categories, Arts & Entertainment, Autos & Vehicles, Beauty & Fitness, Books & Literature, Business & Industrial, Computer & Electronics, Finance, Food & Drink, Games, Health, Hobbies & Leisure, Home & Garden, Internet & Telecom, Jobs & Education, Law & Government, News, Online Communities People & Society, Pets & Animals, Real Estate, Reference, Science, Shopping, Sports, Travel.] For this purpose, we searched for the abbreviations of the major political parties (e.g., CDU, SPD, etc.) using a time window of January 1st to September 26th for all four elections and setting geographic location to Germany. Thereby, it became apparent that the Law & Government category was the most suitable and also the most reasonable for our purposes, as it places search queries in the political context. Previous studies have mostly chosen the "All categories" category which performs significantly worse than "Law & Government" in the German context. We refer the reader to Appendix A1 for the corresponding statistics.

### Search terms

Fundamentally, we assume that google searches for political parties and politicians may reflect vote intentions and choice. Therefore, we can use these searches to predict the latter. Importantly, in pursuing the steps described below we always filter searches using the category described above. @tbl-2 depicts the final search queries that we identified following the steps below. In Step 1, we try out different search terms and explore their popularity over the entire period of four years until the election as depicted in @fig-1. If a search term reaches a peak shortly before the election, we assume that this search interest peaking before an election is an indicator which represents an election intention. In @fig-1 an example is the search term "CDU" and "Angela Merkel" as well as "Armin Laschet" in the 21' election after Angela Merkel resigned. Following this process, we found that across the different parties, the query of the party abbreviation, the full party name and the respective top candidate had peak values before the election. @fig-A2 provides more examples for the other parties.

```{r}
#| label: fig-1
#| fig-cap: "Search queries for the parties (CDU and Die Linke)"


# Figure 1: Search terms ####

# Collect data
# terms_CDU <- gtrends(keyword=c("CDU", "CSU", "Angela Merkel", "Christlich demokratische Union","Armin Laschet"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
# terms_Linke <- gtrends(keyword=c("Linke", "Die Linke","Die Linken"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
# save(terms_CDU, file = paste0("termsCDU_", gsub(":|\\s", "_", Sys.time()), ".RData"))
# save(terms_Linke, file = paste0("termsLinke_", gsub(":|\\s", "_", Sys.time()), ".RData"))

# Load data
load(file = "termsCDU_2023-02-27_11_16_30.RData")
load(file = "termsLinke_2023-02-27_11_16_34.RData")



######### Search terms + period
#Bundestagswahlen: 26.09.2021; 24.09.2017; 22.09.2013; 27.09.2009; 18.09.2005
# Specifying vertical X intercepts for election dates
elec_vlines <- as.Date(c("2009-09-27", "2013-09-18", "2017-09-24", "2021-09-26")) # "2005-09-18", 

# CDU

terms_CDU_df <- terms_CDU$interest_over_time
terms_CDU_df <- terms_CDU_df %>%
  mutate(hits = as.numeric(hits), date = as.Date(date)) %>%
  replace(is.na(.), 0)

###Used in Paper
p1 <- ggplot(terms_CDU_df, aes(x=date, y=hits, group=keyword, col=keyword)) + 
  geom_line(size=0.5)  +
  scale_y_continuous(breaks = seq(0,100, 10)) +
  geom_vline(xintercept = elec_vlines, col= "black", linetype="dotted", size = 1) +
  #theme_minimal(base_size = 22) +
  ylab("Searches (100 = max. interest in time period/territory)") +
  xlab("Date") +
  labs(colour = "Search terms (below)") +
  scale_x_date(date_labels = paste0("%y", "'"),
               date_breaks = "1 year",
               limits = as.Date(c("2006-01-01", "2021-12-01"))) +
  theme(legend.position = "top",
        axis.title.x=element_blank(),
        axis.title.y=element_blank()) +
  annotate(geom = "text", 
           label = paste0("Election: ", elec_vlines), 
           x=elec_vlines+60, 
           y=rep(100, length(elec_vlines)),
           angle = 90,
           hjust = 1,
           size = 2)





# Linke
terms_Linke_df <- terms_Linke$interest_over_time
terms_Linke_df <- terms_Linke_df %>%
  mutate(hits = as.numeric(hits), date = as.Date(date)) %>%
  replace(is.na(.), 0)

###Used in Paper
p2 <- ggplot(terms_Linke_df, aes(x=date, y=hits, group=keyword, col=keyword)) + 
  geom_line(size=0.5)  +
  scale_y_continuous(breaks = seq(0,100, 10)) +
  geom_vline(xintercept = elec_vlines, col= "black", linetype="dotted", size = 1) +
  #theme_minimal(base_size = 22) +
  ylab("Searches (100 = max. interest in time period/territory)") +
  xlab("Date") +
  labs(colour = "Search terms (below)") +
  scale_x_date(date_labels = paste0("%y", "'"),
               date_breaks = "1 year",
               limits = as.Date(c("2006-01-01", "2021-12-01"))) +
  theme(legend.position = "top",
        axis.title.x=element_blank(),
        axis.title.y=element_blank()) +
  annotate(geom = "text", 
           label = paste0("Election: ", elec_vlines), 
           x=elec_vlines+60, 
           y=rep(100, length(elec_vlines)),
           angle = 90,
           hjust = 1,
           size = 2)

result <- p1 + p2 + 
  plot_layout(ncol = 1)
result <- patchwork::patchworkGrob(result)


gridExtra::grid.arrange(result,
                        bottom=grid::textGrob("Date",
                                              gp=grid::gpar(fontsize=12)), 
                                            left=grid::textGrob("Searches (100 = max. interest in time period/territory)", gp=grid::gpar(fontsize=12), rot=90))

# ggsave(plot = plot_searchterms,
#        filename = "Figure_1_searchterms.png", # e.g. change to pdf
#        width = 14,
#        height = 10,
#        device = "png", # e.g. change to pdf
#        dpi = 600) 

```

In Step 2, we compared all search terms for a party identified in Step 1 to explore how relevant the single search terms were in relation to the other search terms. The comparison showed that the party abbreviation, e.g., SPD, is extremely well suited whereas the full party name is not. Special cases were the parties "Die Grünen" and "Die Linken". In the case of the party "Die Grünen", the search query "Grüne" delivered the best results, since the use of a category assures that only search terms in the political context are used and the term already includes longer and other spellings such as "Die Grünen" or "Bündnis 90 die Grünen". As depicted in Figure 1, the same can be observed for the party "Die Linke", for which the search query "Linke" is best suited. In addition, using the same strategy as above we found that the names of the respective leading party candidates yielded even better predictions for all parties. In the case of dual leadership, both candidates were added.^[A special case was the party “Die Linke,” which nominated eight top candidates in 2013. By comparing the search queries of the individual top candidates separately, we ﬁltered out the only two top candidates that seemed relevant for the prediction, who were "Gregor Gysi" and "Sarah Wagenknecht."] The ﬁnal search queries of a party are made up of the party abbreviation and the top candidate(s), which are linked with the "+" operator. This functions according to the scheme of an OR operation, i.e., it sums up the search interest for each individual term. In step 3, we compared the predictive power of a set of search terms containing both the party abbreviation and the top candidate with another set containing only the party abbreviations. We found that the additional consideration of the top candidates provided signiﬁcantly better results.^[Regarding the number of terms within a search query for a party, one could argue that the result of a search query should be divided by the number of search terms, because the search queries of some parties contain more individual terms and could thus be overrepresented (e.g. if a party has a dual leadership). In our opinion, this argumentation is not plausible, because the search terms for one party never provide an equal share of the search interest. The share of the term for the party abbreviation is much higher than that for the party candidates, so it is illogical to treat them equally. We checked this scenario by searching only for the more popular candidate when there is a dual leadership, which results in the same amount of search terms for each party. This, however, leads to much worse predictions than our main analysis. Thus, we assume that the parties are not overrepresented by their multiple top candidates.] @tbl-2 below provides an overview of the ﬁnal search terms.



```{r}
#| label: tbl-2
#| tbl-cap: "Final search queries"
#| tbl-colwidths: auto

data_models %>%
  select(model_name, election_date, GT_keywords) %>%
  group_by(election_date) %>%
  filter(row_number()==1) %>%
  select(election_date, GT_keywords) %>%
  mutate(GT_keywords = sapply(GT_keywords, paste, collapse = " and ")) %>%
  kable(col.names = c("Election date", "Search queries"), booktabs = TRUE) %>%
  kable_styling(font_size = 10)  %>%
      column_spec(2,width = "5in")
```



## Collecting GT data

To collect data on GT, we use the R package "gtrendsR" (Massicotte & Eddelbuettel, 2023). For the purpose of our analysis we rely on the comparison function of GT. In the following, the term "request(s)" implies that we collect data from GT using this function. Unfortunately, Google Trends only allows us to compare up to ﬁve groups of terms at a time. In Germany, however, the total number of major political parties in Germany amounts to six since the appearance of the party AfD in 2013. For the respective elections (2013-2021) we conducted two requests: The ﬁrst request included our search queries for the major political parties CDU, SPD, FDP, "Die Grünen" and "Die Linke." The second request is similar to the ﬁrst one, with the diﬀerence that we exchanged "Die Linke" with AfD, in order to get data for the AfD. Since, the relative scale for the comparative GT data is always anchored (setting the maximum) using the most popular search term, we can leave out or exchange search terms as it does not change their popularity relative to the maximum. As a result we get estimates of search interest for all six parties. We collected Google Trends data, having set the geo-location to Germany, for our search queries from the first day of an election year until the 26th September for 2009, 2013, 2017 and 2021. Additionally we collected data from the election year 2005, which we later used to construct a weighting factor.^[Choosing a period longer than 270 days for the election our years, results in getting weekly data instead of daily data. One might also ask if it would make a difference to pull one dataset that covers all interval and distance combinations, as in our case, or to pull single datasets for each interval and distance combination. We checked this and it made no difference since we only use the comparison function of Google Trends. Accordingly, the ratio of the parties to each other remains the same [@eichenauer_obtaining_2022].]

### Google Trends samples

Importantly, Google Trends draws new samples of all searches on the platform several times a week [@Behnen2020-tf, @eichenauer_obtaining_2022, @raubenheimer_hey_2021]. As a result, the data varies slightly from sample to sample. Google states that the samples taken are representative of all Google search queries. However, Google does not provide any information on how this representativeness is achieved [@google_trends_help_faq_2023]. Since Google does not specify at what intervals new samples are taken, we collected new data sets every hour for several weeks and compared them. It turned out that new samples were taken at least once a day, sometimes more often. To account for the variation one may find across samples, we collect GT data at 10 different time points between 01/12/22 and 10/12/22 (see Table A1 in Appendix A.2). We compare datasets across time points to assure that we base our estimates on non-identical datasets as to account for sampling error. In our data analysis, we then use the average values across those datasets as well as the associated confidence intervals. Surprisingly, we are the first study within the election prediction literature to acknowledge this problem, although it is an obvious source of error [@Behnen2020-tf, @eichenauer_obtaining_2022].  @fig-A2 in Appendix A.2 visualizes the variation of the 10 GT datasets. Moreover, in contrast to statements in the literature referring to data from the Google Trends website [@Stephens-Davidowitz2014-qr] and the GTEfH API [@raubenheimer_hey_2021, @Raubenheimer2022-xe], we find that sometimes more than one different sample can be obtained on the same day (see Table A1 in Appendix A.2) and that drawing samples from different PCs in different networks at the same time yields the same sample when using gtrendsR.

## Collecting polling and election data

In addition to Google Trends data we also collected polling data both as a comparison for our Google Trend predictions but also to weight our Google Trends predictions. The polling data comes from one of the most reputable German polling institutes, Infratest dimap. Finally, the outcome we want to predict are actual election results. We collected the corresponding data from the oﬃcial source of the federal government (Bundeswahlleiter 2023).

## Google trends data windows

After having collected data from 1 January to 27 September of each election year, the question arises which data windows within these data are best suited for election prediction. As depicted in @fig-2, data windows are defined by their width (length of time period) and distance to the election. 

```{r}
#| label: fig-2
#| fig-cap: "Logic of Google Trends data windows"

knitr::include_graphics("Figure_2_data_window_logic.pdf")
```


In previous research authors rarely provided a justification for the corresponding choices. The majority of studies used time window(s) with a width of 1 or 2 weeks / 1 or 3 months ending one day before election/referendum. Our aim is to systematically examine as many data windows as possible in order to find the best possible prediction windows and to provide guidance for future research. In doing so we compare small with large data windows. Small data windows better reflect searches in short time spans and better reflect the current mood in those time periods. Wider data windows reflect average interest across a longer time period and average out non-meaningful spikes (e.g., TV appearances) as well as potentially covering more relevant events. For instance, we could chose a wide, several-week data window that covers both searches from in-person voters right before the election but also searches from mail-in voters a few weeks before. We compare data windows of different width namely 7, 14, 21, 28, 42, 56, 70 and 91 days. Another essential aspect is the distance of our GT data windows to the election. We could base our predictions on a data window that is far from the election, corresponding to a large distance, or a data window that ends right before the election. Previous studies have barely examined this aspect and have mostly chose a distance of one day to the election. However, election campaign leaders and demoscopists will be more interested in how the parties are performing, e.g. two weeks or three months before the election. Furthermore, it is of interest to compare which data windows or models are best suited at which point in time. One assumption could be that polls are more suitable shortly before the election, but GT data with a longer distance to the election. For these reasons, we compare the predictive power of GT data windows of distance one to 150 days before election day where a distance of, e.g., 30 days means that the GT data window ends 30 days before the election. In total, we compare 8 (number of widths) \* 150 (number of distances) = 1200 different GT data windows.

## Prediction models and benchmark {#sec-models}

In our analysis, we predict party shares in four federal elections in Germany: 2009, 2013, 2017 and 2021. We build three broad classes of predictive models: Our first class of models, MC1, consists of the raw, unweighted Google Trends data; The second class of models, MC2, uses a weighting factor based on the results of the preceding election; Our last class of models, MC3, weights the GT predictions using polling results of Infratest dimap, which will be explained in more detail below. In total we estimate 4 (elections) \* 3 (model classes) \* 1200 (data windows) = 14400 models. We compare predictions of models based on GT data to predictions based on models drawing on polls for the time windows to benchmark their performance.^[The term model does not necessarily refer to a sophisticated statistical model here. For instance, the models based on polls are simply the party shares as measured in the polls.]
For each election, we compare our predictions with the actual election results and calculate the average deviation of our predictions from the election results. The calculated average absolute deviations serve as a performance indicator for the different models. MC1 models predict party shares solely with Google Trends data. To obtain these we proceeded as follows: We started by calculating the Google proportion for each party p for an interval i. As shown in Eq. 1, in the ﬁrst step, we use the average Google search interest of each party p for the examined data window i, for example, the average search interest for the search query of the CDU for the data window with width 7 days and distance 7 days. Then we divide it by the sum N of all parties average search interest for that data window. The Google proportion thus serves as a prediction in percent for the respective party.

Google Proportionp,i= Avg. Google search interestp,ip=1NAvg. Google search interestp,i (Eq. 1)

If we add up the Google proportions of all the parties examined, we arrive at 100%. This Google proportion serves as the sole basis for models of class MC1. Highlighting our strategy reveals a slight disadvantage of our analysis data compared to the election polls. While the election polls also include the category "Other," which in extreme cases can go up to 8.7% of the votes, we lack this category on GT. This means that in the polls the main parties can sometimes only get 91.3%, whereas in our analysis they get 100%. As a result, we assume that party shares are slightly overestimated in the GT data. We provide additonal analyses constructing an "other" category from google search data as described in Appendix A.5. We concluded, however, that ignoring the "other" category is fine. In addition, in our conclusion, we discuss possible ways of how the "other" category could be accounted for when collecting GT data. For models of class MC2 and MC3 we use weighting factors in conjunction with the party shares predicted by GT proportions. Models of class MC2 are based on the approach of Polykalas et al. [@polykalas_algorithm_2013, @polykalas_general_2013], who used the results of the previous federal election to calculate a weighting factor. As shown in Eq. 2, to calculate the weighting factor WMC2 for a party p, for the data window i and the election year T, we divide the previous election results of that party p of the previous election year T-4 by the respective Google proportion. For example, to predict the SPD's share in 2017 we must first calculate the weighting factor WMC2. For that, we take the SPD's election result in 2013 and divide it by the GT proportion of the SPD for the election of 2013 using the same distance d and width w for the respective data window i. Subsequently, the GT prediction of the SPD's 2017 share, is multiplied by the weighting factor WMC2 of the SPD. The result is the prediction provided by M2 for the SPD's share in 2017.

WeightMC2(p,i,T)=Previous election resultsp,T-4Google proportionp,i,T-4 (Eq. 2)

WeightMC2(p,i,T) tries to account for the possible selection that characterizes individuals that end up in the GT data by including information on shares in the previous election (4 years earlier) in Germany. For instance, non-Internet users are probably underrepresented and younger voters overrepresented among GT users. Because the electorate changes for each election, we cannot assume that this weighting method controls/offsets the complete sample bias for the election year we want to predict. But it should provide better predictions than models of class MC1 that are solely based on GT data.^[A special case is MC2 for the 2013 election, for which the 2009 election is used to calculate the weighting factor. The party AfD was founded in 2013, as a consequence, we do not have data from 2009. We circumvented this problem by not weighting the AfD’s Google proportion in 2013, which should be taken into account when looking at the results.] In our opinion, this class of models is justified insofar it uses GT and previous election data for the prediction and no other external data.


```{r}
#| label: fig-3
#| fig-cap: "Weighting logic underlying models of class MC3"

knitr::include_graphics("Figure_3_weighting_logic.pdf")
```


For models of class MC3, we use polling data from the Infratest Dimap institute for weighting, which is normally published every two weeks.^[Occasionally in 1 or 3 week intervals.] This enables us to calculate a new weighting factor every two weeks, which corrects for possible over- or underestimates in the GT data. This is especially true for short-term trends due to specific events (e.g. television appearances) that lead to an increase in search queries. The weighting factor is calculated similarly to the previous weighting factor, except that in this case we are looking at slices of two weeks of data (the period from one survey to the next).^[In the actual  data the polls do not appear exactly every two weeks.] The weighting scheme underlying MC3 models is illustrated in @fig-3. @fig-3 depicts a data window that contains three election polls, Poll 1-3. Poll 1-3 slice the data window into Time periods 1-4. Moreover, there is polls before the data window namely Poll X1 and X2. We always weight the GT data (proportions) of a time period, e.g., Time Period 2 with a weighting factor. As shown in Eq. 3, the weighting factor consists of the poll at the start of this time period (here Poll 1) divided by the GT data (proportions) of the previous time period (here Time Period 1). We do the same for the other time periods within the data window, i.e., Time Periods 2 and 3.

WeightMC3(t ‐ t+1)=PollTime Period xGoogle proportionTime Period x - 1 (3)

Time Period 1 is special insofar that in most cases the start date of the time window and a poll do not lie on the same date. Therefore the Google proportion between the start of the interval and the 1st poll (here Poll 1) in the time window cannot be weighted. Therefore, we utilize the last poll before the time window (here Poll X2) to calculate the weighting factor. The Google Proportion needed for the weighting factor is thereby limited by the two last polls before the time window (here Poll X1 and X2, resulting in Time Period X).^[If there is no poll in our data window, we use the first poll before the data window. In the case where the start time of the data window and a poll fall on the same day, the Google proportion up to the 1st poll before the data window is used and divided by the poll falling on the same day as the start time to calculate the weighting factor. Subsequently, the Google proportion starting one day after the start date and poll up to the first poll in the interval is weighted. Polls that fall on the same day as the end day of an interval are ignored because it wouldn't make sense to calculate a weighting factor using only one day of Google data and applying it on the same day.]

# Results

## Comparing predictions across GT data windows, across parties

We start by comparing MC1 models that solely based on GT data, varying the GT data windows both in terms of width, i.e., the number of days covered by the respective GT dataset, and distance, i.e., the number of days the window is away from the election.

```{r}
#| label: fig-4
#| fig-cap: "Accuracy of GT predictions for different parties and party shares across data windows"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"


## Figure 4: Comparison data windows ####


### Data for plots ####
data_plot <- data_predictions_final_mean %>%
  #filter(election_date=="2017-09-24"|election_date=="2013-09-22") %>% #can filter for better overview
  mutate(model_time_interval_fac = factor(as.numeric(model_time_interval, "days"))) %>% # convert to days
  mutate(model_time_interval_fac = paste(model_time_interval_fac, " days", sep="")) %>%
  mutate(model_time_distance = election_date - GT_end_date) %>%
  mutate(model_time_interval_fac = factor(model_time_interval_fac,
                                          levels = c("7 days", "14 days", "21 days", 
                                                     "28 days", "42 days", "56 days", 
                                                     "70 days", "77 days", "84 days", "91 days"),
                                          ordered = TRUE))


cols <- c("SPD" = "red", 
          "CDU" = "black", 
          "AFD" = "blue", 
          "FDP" = "orange", 
          "Linke" = "purple", 
          "Grüne" = "green")


linetypes <- c("SPD" = "solid", 
               "CDU" = "solid", 
               "AFD" = "solid", 
               "FDP" = "solid", 
               "Linke" = "solid", 
               "Grüne" = "solid")




data_plot1 <- data_plot %>%
  filter(datasource_weight =="GT") %>%
  group_by(model_name) %>%
  #mutate(group_mean_deviation = mean(abs(Mean_dev))) %>%
  filter(election_date == "2021-09-26") %>%
  filter(model_time_interval_fac == "7 days" |
         model_time_interval_fac == "14 days" |
         model_time_interval_fac == "28 days" |
           model_time_interval_fac == "91 days") %>% 
  ungroup() %>%
  mutate(model_time_interval_fac = recode(model_time_interval_fac,
                                          "7 days" = "Plot 1:\n7 days",
                                          "14 days" = "Plot 2:\n14 days",
                                          "28 days" = "Plot 3:\n28 days",
                                          "91 days" = "Plot 4:\n91 days"))



# Create x-axis tick labels
x_tick_labels <- data_plot1 %>%
  group_by(GT_end_date) %>%
  filter(row_number()==1) %>%
  ungroup() %>%
  slice(c(1,25, 50, 75, 100, 125, 150)) %>%# Pick every 30th row
  select(GT_start_date, GT_end_date, model_time_distance)



ggplot(data_plot1,
            aes(x = GT_end_date,
                y = Mean_dev,
                color = party#,
                #linetype = party
                )) +
  geom_vline(xintercept = as.Date("2021-09-26"),
             linetype="dashed") +
  geom_hline(yintercept = 0,
             linetype="solid",
             color = "lightgray") +  
  geom_point(size = 0.5) +
  geom_line() +
  theme_minimal(base_size = 22) +
  facet_grid(vars(model_time_interval_fac),
             scales = "free_x") +
  scale_x_date(breaks = x_tick_labels$GT_end_date,
               labels = paste0(x_tick_labels$model_time_distance, " day(s)\n[", x_tick_labels$GT_end_date, "]")
  ) +
  scale_y_continuous(sec.axis = dup_axis(
    name = "Width of data window")) +
  scale_color_manual(values = cols) + 
  scale_linetype_manual(values = linetypes) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position="top",
        axis.text.y.right = element_blank(),
        strip.background =element_rect(fill="white")) +
  ylab("Deviation on % scale\n(prediction error)") +
  xlab("Distance of data window [end date of window]") +
  labs(colour = "Party")
# stat_summary(aes(y = group_mean_deviation, group = 1), fun=mean, colour="purple", geom="line", linetype = "dashed")
#+ geom_errorbar(aes(ymin=dev_lower.ci, ymax=dev_upper.ci),width=.3, position=position_dodge(.9))

# Data for text (@fig-4)
n_predictions <- nrow(data_plot1)
n_parties <- length(unique(data_plot1$party))
n_models <- length(unique(data_plot1$model_name))
n_widths <- length(unique(data_plot1$model_time_interval))
n_distances <- length(unique(data_plot1$model_time_distance))

# Data for text (Footnote)
data_predictions_final_mean_GT <- data_predictions_final_mean %>% filter(datasource_weight=="GT")
n_predictions_all <- nrow(data_predictions_final_mean_GT)
n_parties_all <- length(unique(data_predictions_final_mean_GT$party))
n_models_all <- length(unique(data_predictions_final_mean_GT$model_name))
n_widths_all <- length(unique(data_predictions_final_mean_GT$model_time_interval))
n_distances_all <- length(unique(data_predictions_final_mean_GT$model_time_distance))
n_elections_all <- length(unique(data_predictions_final_mean_GT$election_date))


data_predictions_final_mean_GT_2013_2021 <- data_predictions_final_mean %>%
  filter(datasource_weight=="GT",
         election_date=="2013-09-22" | 
         election_date=="2017-09-24" |
         election_date=="2021-09-26")

n_predictions_2013_2021 <- nrow(data_predictions_final_mean_GT_2013_2021)


data_predictions_final_mean_GT_2009 <- data_predictions_final_mean %>%
  filter(datasource_weight=="GT",
         election_date=="2013-09-22" | 
         election_date=="2017-09-24" |
         election_date=="2021-09-26")

n_predictions_2009 <- nrow(data_predictions_final_mean_GT_2009)




```



In @fig-4 we visualize a subset of the predictions provided by MC1-GT models for the most recent general election in Germany (26th of September 2021). @fig-4 plots `r n_predictions` deviations = predictions - actual vote shares^[Obtained by averaging predictions over 10 GT data samples. CHECK!] across parties as points connected by lines, namely predictions for  `r n_parties` parties of based on `r n_models` models defined by `r n_widths` widths and `r n_distances` distances.^[The graph is only showing a subset of our predictions restricted to one election and four data window widths. Overall, from our models that are solely based on GT data we yielded `r n_predictions_all` deviations = predictions - actual vote shares, namely predictions for  `r n_parties_all` parties of based on `r n_models_all` models defined by `r n_widths_all` widths, `r n_distances_all` distances and `r n_elections_all`. For the 2009 election, we have `r n_predictions_2009` solely GT data based predictions. For the 2013, 2017, 2021 elections we obtained `r n_predictions_2013_2021` 27600 predictions.] 


The most recent election is ideal for benchmarking GT predictions, because usage of the platform has changed over time. Connecting lines have been added to facilitate comparison. The single panels (Plot 1-4) correspond to the four different widths of the data windows going from 7 days to 91 days (see right-hand y-axis). The x-axis indicates the distance of the respective window to the election. First, we find that the width and distance of the data window to the election matters. We compare models of differing width (from top to bottom) and distance (from left to right). Data windows of low width, e.g., the models based on 7 days of GT data in the top plot, vary much stronger in their predictive accuracy across time. In other words, with such small time spans it matters which week of GT data we have picked for our prediction. Naturally, this variation decreases when we extend the time window on which we base our prediction going from 7 days (Plot 1) to 91 days (Plot 4) at the bottom in @fig-4. Second, on average accuracy seems to improve slightly the smaller the distance between the GT data window and the election. In @fig-5 (next section) we plot the prediction error averaged across all parties for MC1-GT models (red line) contrasting them with other models. Focusing on the red line this analysis also indicates that the error decreases the closer we are to the election. However, we can also clearly see that picking a short data window, e.g., 7 days, results in considerable variation in what regards accuracy (see @fig-4, Plot 1). In addition, we used a linear model to model the trend of accuracy as a function of distance holding the width constant at 14 days in @tbl-A2. For the 2021 election, the average deviation decreases by 0.02% per day distance. In other words, if move the data window 100 days closer to the election, the mean absolute deviation decreases by 2%. This, however, is not true for the other elections (cf. @tbl-A2). Third, @fig-4 also highlights the strong variation in accuracy between parties. For instance, GT predictions of the vote share of the Linke are more accurate than, e.g., for the AFD, with the deviation being closer to 0 across models. For the 2021 election the error generally seems highest for the AfD. Moreover, shares of certain parties are usually underestimated (e.g., SPD), while others are overestimated (e.g., AfD). It seems that the quality of Google searches as a signal of vote choice differs across parties. Importantly, in @fig-A4 provides further visualizations for more intervals for the 2021 election, @fig-A5 provide visualizations for the other elections and @tbl-A2 also provides models for the other elections. The insights described above are largely confirmed when using data for other elections.

## Comparing predictions across model classes

Above, we compared different models within class MC1-GT focusing on the width and distance of data windows. Previous studies have often combined GT data with election data or polls. As described in @sec-models we compare predictions across three broad classes of models. @fig-5 again focuses on the 21' election. 


```{r}
#| label: fig-5
#| fig-cap: "Accuracy of GT predictions for different parties and party shares across data windows"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"





## Figure 5: Comparison models ####
# prediction error averaged across all parties for the GT data + other datasources

# Create average prediction error (across all parties) ###
###Plot GT vs. Polls
for(i in as.character(unique(data_plot$election_date))){ # Loop over elections
  #print(i)
  data_plot2 <- data_plot %>%
  filter(datasource_weight =="GT" | 
           datasource_weight =="Infratest" |
           datasource_weight =="GT + election weight" |
           datasource_weight =="GT + weekly polls weight"
  ) %>%
  group_by(model_name) %>% 
  mutate(deviation_mean = mean(abs(Mean_dev), na.rm=TRUE)) %>%
  filter(election_date == i) %>%
  filter(model_time_interval_fac == "7 days" |
           model_time_interval_fac == "14 days" |
           model_time_interval_fac == "28 days" |
           model_time_interval_fac == "91 days") %>% 
  ungroup() %>%
  mutate(datasource_weight = recode(datasource_weight, 
                                    "GT" = "MC1: GT",
                                    "GT + election weight" = "MC2: GT + election weight",
                                    "GT + weekly polls weight" = "MC3: GT + weekly polls weight"))

#  WHY NOT AGGREGATE DATASET?

# Create x-axis tick labels
x_tick_labels <- data_plot2 %>%
  group_by(GT_end_date) %>%
  filter(row_number()==1) %>%
  ungroup() %>%
  slice(c(1,25, 50, 75, 100, 125, 150)) %>%# Pick every 30th row
  select(GT_start_date, GT_end_date, model_time_distance)


# Count number of models
# data_plot2 %>% group_by(datasource_weight) %>% summarize(n_models = n())




cols2 <- c("MC1: GT" = "#e41a1c", 
           #"Only polls" = "black", 
           "Infratest" = "black",
           "MC2: GT + election weight" = "#984ea3",
           "MC3: GT + weekly polls weight" = "#ff7f00")


if(i == "2021-09-26"){ # Only for 21# eleection
  

# Compare predictions across models
# Table profide the number/percentage of models where MC1-3 beat polls

model_comparison <- data_plot2 %>% 
  select(model_time_interval, model_time_distance, datasource_weight, deviation_mean) %>%
  group_by(datasource_weight) %>% 
  mutate(id = row_number()) %>%
  pivot_wider(names_from = datasource_weight, values_from =  deviation_mean) %>%
  group_by(model_time_interval, model_time_distance) %>%
  slice(1) %>% 
  ungroup() %>%
  mutate(id = row_number()) %>% # Recreate ID (aggregated)
  mutate(polls_vs_GT = ifelse(`Infratest` < `MC1: GT`, TRUE, FALSE),
         polls_vs_GTE = ifelse(`Infratest` < `MC2: GT + election weight`, TRUE, FALSE),
         polls_vs_GTP = ifelse(`Infratest` < `MC3: GT + weekly polls weight`, TRUE, FALSE))

# TRUE = POLLS ARE BETTER, FALSE = MODELCLASS X is BETTER
model_comp <- function(var){
  result <- model_comparison[var]
  result <- paste0(round(prop.table(table(model_comparison[var])),2)[1]*100, 
                   "% (",
                   table(model_comparison[var])[1],
                   " out of ",
                   sum(table(model_comparison[var])),
                   ")")
}



comparison_MC1_GT <<- model_comp("polls_vs_GT")
comparison_MC2_GTE <<- model_comp("polls_vs_GTE")
comparison_MC3_GTP <<- model_comp("polls_vs_GTP")

# Comparison only for large width
# TRUE = POLLS ARE BETTER, FALSE = MODELCLASS X is BETTER
model_comparison_large_width <- model_comparison %>% 
  filter(model_time_interval == "7862400s (~13 weeks)")

comparison_MC2_GTE_large_width <<- model_comp("polls_vs_GTE")
comparison_MC1_GT_large_width <<- model_comp("polls_vs_GT")
comparison_MC3_GTP_large_width <<- model_comp("polls_vs_GTP")


}

# Add Plot X labels
data_plot2 <- data_plot2 %>%
  mutate(model_time_interval_fac = recode(model_time_interval_fac,
                                          "7 days" = "Plot 1:\n7 days",
                                          "14 days" = "Plot 2:\n14 days",
                                          "28 days" = "Plot 3:\n28 days",
                                          "91 days" = "Plot 4:\n91 days"))




#Plot GT vs. Polls
figure_5 <- ggplot(data_plot2,
             aes(x = GT_end_date,
                 y = deviation_mean,
                 color = datasource_weight)) +
  geom_vline(xintercept = as.Date("2021-09-26"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2017-09-24"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2013-09-22"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2009-09-27"),
             linetype="dashed") +
  geom_hline(yintercept = 0,
             linetype="solid") +  
  geom_point(size = 0.5) +
  geom_line() +
  theme_minimal(base_size = 22) +
  facet_grid(vars(model_time_interval_fac),
             #vars(election_date), 
             scales = "free_x") +
  scale_x_date(breaks = x_tick_labels$GT_end_date,
               labels = paste0(x_tick_labels$model_time_distance, " day(s)\n[", x_tick_labels$GT_end_date, "]")
  ) +
  scale_y_continuous(sec.axis = dup_axis(
    name = "Width of data window")) +
  scale_color_manual(values = cols2) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position="top",
        axis.text.y.right = element_blank(),
        plot.caption=element_text(hjust = 0),
        strip.background =element_rect(fill="white")) +
  labs(x = "Enddate of interval\n(= distance)",
       y = "MeanDeviation in %\n(prediction error)",
       colour = "Model class",
       caption = paste0("Note: Predictive models for ",i," election."),
       title = paste0("Election: ", i))
#+ geom_errorbar(aes(ymin=dev_lower.ci, ymax=dev_upper.ci),width=.3, position=position_dodge(.9))

assign(paste0("Figure_5_",gsub("-", "_", i)), figure_5)

}


Figure_5_2021_09_26


```

Now, each point represents the mean absolute error, i.e., the prediction error averaged across parties whereby we obtain 600 predictions per model (+ 600 for Intratest) and the models are colored: MC1 models based on GT data in red, MC2 models based on GT data weighted with previous election results in purple, MC3 models based on GT data weighted with polling data in orange. Finally, we have the predictions based on Infratest Dimap polls, whereby we simply use last Dimap poll in the respective time windows. As in @fig-4, the various plots in @fig-5 (Plot 1-4) correspond to different widths of the data window.
No surprisingly, we find that polls (black line) unarguably perform the best for the 2021 election reflected in the fact that prediction error mostly scores below our GT data based models of Class MC1-3. In @fig-3 this becomes visible as the black line is almost always closer to 0 then the other ones.          
We also compared how often our other models actually provide better predictions. GT data based MC1 models provide better prediction in `r comparison_MC1_GT` of the cases. MC2 models that weight GT data with previous election perform the worst with `r comparison_MC2_GTE` better predictions as compared to polls by Infratest Dimap. Finally, MC3 models that combine GT data with polls perform the best with `r comparison_MC3_GTP` of predictions being better. The picture hardly changes when we restrict our models to those based on width 91 days.           
Naturally, we could discuss whether the accuracy of our GT based models (MC1) is really so much worse than polling data. Given that we can collect GT data for free, errors of the size found here may be acceptable for certain applications where we only need rough predictions. Besides, if we want to decrease the dependency of GT-based predictions on short-term trends we should probably base our predictions on a larger time span as shown in Plot in @fig-3. We provide findings for other elections in the next section and in @sec-comparing-model-classes-other.

## Comparing predictions across model classes and elections

Above we focused on the most recent election in Germany (9th of September 2021). This focus is justified: Given the changing user population, the most recent election is the most appropriate to assess the predictive power of GT search data. Nonetheless, a comparison with earlier elections can provide us with some insights as to what variations we can expect for coming elections. @fig-6 is similar to @fig-5 only that the width of the data window is held constant at 91 days which corresponds to Plot 4 in @fig-5, and predictions for different elections are shown on top of each other (Plot 1-4). Again, each point represents the mean absolute error, i.e., the prediction error averaged across parties whereby predictions by different model classes are colored.      


```{r}
#| label: fig-6
#| fig-cap: "Accuracy of predictions across model classes and election"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"


## Figure 6: Comparison elections ####
data_plot2 <- data_plot %>%
  filter(datasource_weight =="GT" | 
           datasource_weight =="Infratest" |
           datasource_weight =="GT + election weight" |
           datasource_weight =="GT + weekly polls weight"
  ) %>%
  group_by(model_name) %>% 
  mutate(deviation_mean = mean(abs(Mean_dev), na.rm=TRUE)) %>%
  #filter(election_date == i) %>%
  filter(model_time_interval_fac == "91 days") %>% 
  ungroup() %>%
  mutate(datasource_weight = recode(datasource_weight, 
                                    "GT" = "MC1: GT",
                                    "GT + election weight" = "MC2: GT + election weight",
                                    "GT + weekly polls weight" = "MC3: GT + weekly polls weight")) %>%
  mutate(election = factor(election, levels = c("18 Sep, 2005", 
                                                "27 Sep, 2009", "22 Sep, 2013", "24 Sep, 2017", "26 Sep, 2021"),
                           ordered = TRUE)) %>%
  mutate(election = recode(election, 
                           "18 Sep, 2005" = "18 Sep, 2005", 
                           "27 Sep, 2009" = "Plot 1:\nElection\n27 Sep, 2009", 
                           "22 Sep, 2013" = "Plot 2:\nElection\n22 Sep, 2013", 
                           "24 Sep, 2017" = "Plot 3:\nElection\n24 Sep, 2017", 
                           "26 Sep, 2021" = "Plot 4:\nElection\n26 Sep, 2021"))


#  WHY NOT AGGREGATE DATASET?

# Create x-axis tick labels
x_tick_labels <- data_plot2 %>%
  group_by(GT_end_date) %>%
  filter(row_number()==1) %>%
  ungroup() %>%
  slice(c(1,50, 100, 150)) %>%# Pick every 30th row
  select(GT_start_date, GT_end_date, model_time_distance)



# Compare predictions across models
# Table profide the number/percentage of models where MC1-3 beat polls

model_comparison <- data_plot2 %>% 
  select(model_time_interval, model_time_distance, election, datasource_weight, deviation_mean) %>%
  group_by(datasource_weight, election) %>% 
  mutate(id = row_number()) %>%
  pivot_wider(names_from = datasource_weight, values_from =  deviation_mean) %>%
  group_by(model_time_interval, model_time_distance, election) %>%
  slice(1) %>% 
  ungroup() %>%
  group_by(election) %>%
  arrange(election, model_time_interval, model_time_distance) %>%
  mutate(id = row_number()) %>% # Recreate ID (aggregated)
  ungroup() %>%
  mutate(polls_vs_GT = ifelse(`Infratest` < `MC1: GT`, TRUE, FALSE),
         polls_vs_GTE = ifelse(`Infratest` < `MC2: GT + election weight`, TRUE, FALSE),
         polls_vs_GTP = ifelse(`Infratest` < `MC3: GT + weekly polls weight`, TRUE, FALSE))

  

# TRUE = POLLS ARE BETTER, FALSE = MODELCLASS X is BETTER
  # table(model_comparison$election, model_comparison$polls_vs_GT)
  # prop.table(table(model_comparison$election, model_comparison$polls_vs_GT), margin = 1)
  # table(model_comparison$election, model_comparison$polls_vs_GTE)
  # prop.table(table(model_comparison$election, model_comparison$polls_vs_GTE), margin = 1)
  # table(model_comparison$election, model_comparison$polls_vs_GTP)
  # prop.table(table(model_comparison$election, model_comparison$polls_vs_GTP), margin = 1)


# TRUE = POLLS ARE BETTER, FALSE = MODELCLASS X is BETTER
model_comp_election2 <- function(var1, var2){
  # var1 <- "election"
  # var2 <- "polls_vs_GT"
  result_table <- table(model_comparison %>% pull(var1), 
                        model_comparison %>% pull(var2)) %>%
    as.data.frame.matrix() %>%
    slice(-1)
  result_proptable <- prop.table(table(model_comparison %>% pull(var1), 
                        model_comparison %>% pull(var2)),
                        margin = 1)  %>%
    as.data.frame.matrix() %>%
    slice(-1) 
  result <- paste(paste0("Election ", 
         str_extract(rownames(result_proptable), "[0-9][0-9][0-9][0-9]$"), 
         ": ",
         round(result_proptable[,1],2)*100, 
         "% (", result_table[,1], 
         " out of ", 
         result_table[,1]+result_table[,2], ")"), 
        collapse=", ")
  
  results_table <- 
    bind_cols(Model = var2, 
              Election = str_extract(rownames(result_proptable), "[0-9][0-9][0-9][0-9]$"),
              "Better predictions: %" = paste0(round(result_proptable[,1],2)*100, "%"),
              "Better predictions: n out of total" = paste0(result_table[,1], " out of ", result_table[,1]+result_table[,2]))
  #print(result)
  results_table
}


table_comparison <- 
  bind_rows(model_comp_election2("election", "polls_vs_GT"),
            model_comp_election2("election", "polls_vs_GTE"),
            model_comp_election2("election", "polls_vs_GTP")) %>%
  mutate(Model = recode(Model,
    "polls_vs_GT" = "MC1: GT",
    "polls_vs_GTE" = "MC2: GT + election weight",
    "polls_vs_GTP" = "MC3: GT + weekly polls weight"
  )) %>% 
  rename("Model class" = Model)

table_comparison[c(2:4, 6:8, 10:12),1] <- ""

  # prop.table(table(model_comparison$election, model_comparison$polls_vs_GTE), margin = 1)
  # table(model_comparison$election, model_comparison$polls_vs_GTP)
  # prop.table(table(model_comparison$election, model_comparison$polls_vs_GTP), 



cols2 <- c("MC1: GT" = "#e41a1c", 
           #"Only polls" = "black", 
           "Infratest" = "black",
           "MC2: GT + election weight" = "#984ea3",
           "MC3: GT + weekly polls weight" = "#ff7f00")

add_election_word <- function(x){paste0("Election:\n",x)}

#Plot GT vs. Polls
ggplot(data_plot2,
             aes(x = GT_end_date,
                 y = deviation_mean,
                 color = datasource_weight)) +
   geom_vline(xintercept = as.Date("2021-09-26"),
              linetype="dashed") +
   geom_vline(xintercept = as.Date("2017-09-24"),
              linetype="dashed") +
   geom_vline(xintercept = as.Date("2013-09-22"),
              linetype="dashed") +
   geom_vline(xintercept = as.Date("2009-09-27"),
              linetype="dashed") +
  geom_hline(yintercept = 0,
             linetype="solid") +  
  geom_point(size = 0.5) +
  geom_line() +
  theme_minimal(base_size = 22) +
      facet_grid2(rows = vars(election), 
             scales = "free",
             independent = "x") + # labeller = labeller(election = add_election_word)
  scale_y_continuous(sec.axis = dup_axis(
    name = "Election")) +
  scale_color_manual(values = cols2) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position="top",
        axis.text.y.right = element_blank(),
        plot.caption=element_text(hjust = 0),
        strip.background =element_rect(fill="white")) +
  labs(x = "Enddate of interval\n(= distance)",
       y = "MeanDeviation in %\n(prediction error)",
       colour = "Model class",
       caption = paste0("Note: Predictive models across all four elections holding the data window width constant at 91 days."),
       title = paste0("Election: All elections"))

```


```{r}
#| label: tbl-3
#| tbl-cap: "Comparing model predictions to polls"
#| tbl-colwidths: auto

table_comparison %>%
  kable(booktabs = TRUE) %>%
  kable_styling(font_size = 9) %>%
      column_spec(4,width = "1in") %>%
      column_spec(3,width = "1in")
```

 
We find that indeed the accuracy of predictions varies across elections. @tbl-3 provides an overview regarding how often the different models of the classes MC1-3 where better than purely poll based predictions across elections (poll based predictions always concern the last poll). We can see that 2021 and 2009 were particular bad years for models of class MC1 and MC2 with 0 better predictions than polls. In contrast, MC3 provided better predictions in for 20% of the estimated models both in 2009 and 2021.
For the 2013 election, the purely GT data based models beat the polls in 66% of the estimated models and provide better predictions than the other model classes most of the time. For the 2017 election,the purely GT data based models fare even better, beating the polls in 97% of the models (defined by width and distance), with the other two model classes faring somewhat worse. Hence, in general, 2017 was a particularly bad year for the polls in our data.         
We can only speculate why there is such strong variation across elections. First, as mentioned, we have to assume that the representativity of Google search users varies across time. In principle, it is possible that the population of Google search users was more representative of German voters in the 2013, 2017 elections than in 2009 or 2021. Second, it could be related to the elections themselves. Potentially, the extend to which the action "search for" is related to "vote for" depends on the election with more "charged" elections like the 2021 election decreasing this correlation [@faas_german_2022]. Third, it is possible that the accuracy of the polling data which we use as a benchmark varies over time.




# Conclusion

Google trends data has become a popular data source for prediction in different domains such as elections. While most previous research focuses on binary electoral outcomes such as referendum results or presidential elections, we evaluate GT predictions in a multi-party setting. Furthermore, we developed a framework that allows us to compare predictions across 1200 fine-grained GT data windows that vary both in terms of width (7 to 91 days) and distance to the election (1 to 150 days). And, we compare predictions across several elections (four general elections in Germany). Besides, we provide a more systematic assessment hitherto neglected choices such as the selection of search terms, GT data samples and search refinement categories. Tackling these different dimensions allows us to provide some unique insights. First, we find that predictive accuracy varies significantly as a function of the width and distance of GT data windows. To some extent disagreement on the predictive accuracy of GT data in previous research may be linked to the varying choices researchers made here (cf. @tbl-2). In what concerns width(s), accuracy varies significantly across time for shorter data windows (here 7-28 days). We would generally recommend choosing larger data windows for predictive purposes to average out variation that is due to singular events. And, akin to classic polling data we find that predictive accuracy increases the closer our data window to the election. Second, in terms of predictive models we compared models based on GT data, on GT + previous electoral outcomes data and GT + opinion poll data. Generally, high quality opinion polls, in our case by the reknowned company Infratest dimap, still represents the benchmark in terms of accuracy, especially in the case of the lates election in 2021. We found that models that combine polling data with GT data (MC3) data fare better than purely GT-data-based models (MC1). Furthermore, we find that weighting GT data with previous electoral outcomes [cf. @polykalas_algorithm_2013, @polykalas_general_2013] does not help predictive accuracy, at least not for the German case (cf. @fig-3). Third, while we argued that recent elections are the appropriate benchmarks for GT data predictions, comparisons to past elections may still be revealing. Especially, insofar our findings for the 2021 election are somewhat discouraging, which goes against earlier studies that emphasize the predictive accuracy of GT data. Hence, we need to test whether our findings for 2021 also hold for other elections. And, especially, for the 2013 and 2017 election GT data models where much more accurate, at times beating or at least aligning with predictions based on opinion polls (cf. @fig-6). Above we speculated that the nature of the election may affect predictive power of GT (cf. Faas and Klingelhöfer 2022). More generally, this highlights that conclusions regarding predictive accuracy from one election may not generalize to other elections. Finally, we conclude that GT prediction research should ideally become more transparent. From our review we learned that identifying the various characteristics in @tbl-2, i.e., all representing important choices when using GT data, was a challenge. Future research would benefit if authors would report more details, e.g., the exact time of GT data collection, the number of GT samples, the nature of search term selection. For instance, we found that it matters which category search filter is selected (cf. Appendix A.1). Our study has several limitations that provide venues for future research. First, we pursued a systematic approach to search term selection, testing different search terms against each other (see Section 3.2). Future research might benefit from choosing an even wider net of search terms, trying out different ones. Second, while we predicted elections shares we did not forecast them in that the elections happened already. Since, we didn't built any sophisticated models we somewhat circumvented the danger of adapting our models to already seen data. Nonetheless, as is now more common in the literature on election forecasting [@gschwend_zweitstimme_2022], studie that rely on GT data for prediction could also be pre-registered. Third, from the general perspective of election forecasting, GT data could be seen as just another dynamic signal that can be integrated into more sophisticated modelling strategies [e.g., @stoetzer_forecasting_2019] that go beyond the simple weighting approach used in the present paper. Whether such additional sources of data can improve predictions remains to be tested. Fourth, the changing predictive power of GT across elections maybe related to how representative the GT data is of searches on the platform (1) and how representative users of the GT platform are of voting population (2). Pertaining to (1), both aspects of representiveness should be studied further. Finding variation across GT data samples Raubenheimer et al. [@raubenheimer_hey_2021] suggested that there might be a publication bias, i.e., successful predictions are based on suitable samples. For this reason we averaged across GT data samples (cf. Section 3.3. and A.2). However, more rearch into the variability of such samples for particular predictions is warranted. However, (2) is relevant as well. Platforms are generally quite secretive about their userbase. Collecting more evidence on how representative Goodle users are of the general (voting) population would help to explain the accuracy of corresponding predictions and help to develop more sophisticated weighting schemes.

\newpage

# Online appendix

\beginsupplement


## Category filter selection and search terms

As discussed in Section 3.2, Google trends allows for restricting searches to certain categories, helpful insofar it allows for restricting the data to Google searchers that are more relevant to the phenomenon we want to predict, i.e., elections. To identify the best category filter, we proceeded as follows: First we defined a time period and geographical context, namely the time window of January 1st to September 26th preceding the respective elections. Second, we defined a set of search terms namely the abbreviations of the major political parties (e.g., CDU, SPD, etc.). We drew 10 different GT data samples at different time points to account for any possible variation across samples. Then for each election year and each sample, we calculated the Google proportion as explained in Section 3.6 and summed the corresponding absolute percentage deviations of each party from the actual election result were summed. @fig-A1 visualizes the average absolute percentage deviation of the Google proportion from the party shares, indicating that restricting searches to the category "Law & Government" provides the best predictions. Hence, in our main analysis we further restricted searches using this category filter.


```{r}
#| label: fig-A1
#| fig-cap: "Category selection among supercategories of Google Trends"
#| fig-width: 14
#| fig-height: 10
#| fig-pos: "H"





###### Run the following code if you want to draw a sample for each category for all four election years ####
#
## get all categories of Google Trends
#data('categories')
#
## filter the corresponding dataset for all supercategories
#categories <- categories %>%
#  filter(name == "All categories" | 
#           name == "Arts & Entertainment" |
#           name == "Autos & Vehicles" |
#           name == "Beauty & Fitness" |
#           name == "Books & Literature" |
#           name == "Business & Industrial" |
#           name == "Computer & Electronics" |
#           name == "Finance" |
#           name == "Food & Drink" |
#           name == "Games" |
#           name == "Health" |
#           name == "Hobbies & Leisure" |
#           name == "Home & Garden" |
#           name == "Internet & Telecom" |
#           name == "Jobs & Education" |
#           name == "Law & Government" |
#           name == "News" |
#           name == "Online Communities" |
#           name == "People & Society" |
#           name == "Pets & Animals" |
#           name == "Real Estate" |
#           name == "Reference" |
#           name == "Science" |
#           name == "Shopping" |
#           name == "Sports" |
#           name == "Travel") %>%
#  filter(!duplicated(name))         # needed because Pets & Animals appear two times in categories list
#
#
## preparation for loop
## create lists containing the election results 
#list_electionresults <- NULL
#list_electionresults[["2009-09-27"]] <- data.frame(party=c("CDU", "FDP", "Grüne", "Linke", "SPD"),
#                                                   share=c(33.8, 14.6, 10.7, 11.9, 23)) %>% arrange(party)
#
#list_electionresults[["2013-09-22"]] <- data.frame(party=c("AFD", "CDU", "FDP", "Grüne", "Linke", "SPD"),
#                                                   share=c(4.7, 41.5, 4.8, 8.4, 8.6, 25.7)) %>% arrange(party)
#
#list_electionresults[["2017-09-24"]] <- data.frame(party=c("AFD", "CDU", "FDP", "Grüne", "Linke", "SPD"),
#                                                   share=c(12.6, 32.9, 10.7, 8.9 , 9.2, 20.5)) %>% arrange(party)
#
#list_electionresults[["2021-09-26"]] <- data.frame(party=c("AFD", "CDU", "FDP", "Grüne", "Linke", "SPD"),
#                                                   share=c(10.3, 24.1, 11.5, 14.8 , 4.9, 25.7)) %>% arrange(party)
#
#
## needed for loop
#cat_name <- as.character(categories$name)
#cat_ids <- as.integer(categories$id)
#years <- c("2021-01-01 2021-09-26", "2017-01-01 2017-09-26", "2013-01-01 2013-09-26", "2009-01-01 2009-09-26")
#
#
## create empty dataset in which the GT data and results are written
#Comp_categories <- data.frame("Category_ID" = cat_ids, "Category_Name" = cat_name, "Data_2021" = NA, 
#                              "Data_2017" = NA, "Data_2013" = NA, "Data_2009" = NA, 
#                              "Elec_results21" = NA, "Dev21" = NA,
#                              "Elec_results17" = NA, "Dev17" = NA,
#                              "Elec_results13" = NA, "Dev13" = NA,
#                              "Elec_results09" = NA, "Dev09" = NA)
#
#
## write previously created election results into dataset 
#for (i in 1:nrow(Comp_categories)){
#  
#  Comp_categories$Elec_results09[i] <- list(list_electionresults[["2009-09-27"]])
#  Comp_categories$Elec_results13[i] <- list(list_electionresults[["2013-09-22"]])
#  Comp_categories$Elec_results17[i] <- list(list_electionresults[["2017-09-24"]])
#  Comp_categories$Elec_results21[i] <- list(list_electionresults[["2021-09-26"]])
#  
#}
#
#
## The following code pulls GT raw data for all categories for the four election years  
## trycatch is used since it possible that http error 429 appears (too many requests) 
## the code is designed in fashion that if you retrieve only an empty dataset that it will retry it 5 times and then proceeds
## In the case that after 5 tries no data was retrieved you can execute the code over and over until all datasets are collected
#
#t = 0
#
#for (o in years){
#  
#  for (i in cat_ids){
#    
#    
#    if (t >= 25){
#      
#      t = 0
#      
#    }
#    
#    l = 0
#    t = t + 1      
#    
#    
#    if (o == "2021-01-01 2021-09-26" & is.na(Comp_categories$Data_2021[t])){
#      
#      repeat{
#      
#      skip_to_next_1 <- FALSE
#      
#      trend_all_cat = tryCatch(gtrends(keyword= c('CDU', 'SPD', 'Grüne', 'Linke', 'FDP'), 
#                                       geo = "DE" , category = i , time = o , gprop = "web", onlyInterest = TRUE), error = function(e) { skip_to_next_1 <<- TRUE})
#      
#      if(skip_to_next_1 == TRUE) { next }     
#      Sys.sleep(5)
#  
#      skip_to_next_2 <- FALSE
#      
#      trend_all_cat_AFD = tryCatch(gtrends(keyword= c('CDU', 'SPD', 'Grüne', 'FDP', "Afd"), 
#                                           geo = "DE" , category = i, time = o , gprop = "web", onlyInterest = TRUE), error = function(e) { skip_to_next_2 <<- TRUE})
#      
#      if(skip_to_next_1 == FALSE & skip_to_next_2 == FALSE & is.null(trend_all_cat[[1]][1]) == FALSE & is.null(trend_all_cat_AFD[[1]][1]) == FALSE) { 
#        
#        break
#      }
#      
#      if(l == 5){
#        
#        
#        break
#        
#      }
#      
#      l = l + 1
#      print("Again")
#      
#      }   
#      
#      
#      Comp_categories$Data_2021[t] <- list(rbind(trend_all_cat$interest_over_time, 
#                                                 anti_join(trend_all_cat_AFD$interest_over_time, trend_all_cat$interest_over_time, by = "keyword")))
#      
#      
#      mem <- as.data.frame(Comp_categories$Data_2021[t]) %>%
#        group_by(keyword) %>%
#        mutate(hits = str_replace(hits, "<1", "0"), # HIER WEITER
#               hits = as.numeric(hits)) %>%
#        dplyr::summarise(mean_hits = mean(hits)) %>%
#        mutate(dev_hits = mean_hits - Comp_categories$Elec_results21[[t]][2],
#               gprop = ifelse(mean_hits >= 1, mean_hits/sum(mean_hits)*100, 0),
#               dev_gprop = gprop -  Comp_categories$Elec_results21[[t]][2],
#               sum_dev_hits_abs = sum(abs(dev_hits$share)),
#               sum_dev_prop_abs = sum(abs(dev_gprop$share)))
#      
#      Comp_categories$Dev21[t] <- list(mem)
#      
#      print(i)
#      print(t)
#      
#    }
#    
#    
#    if (o == "2017-01-01 2017-09-26" & is.na(Comp_categories$Data_2017[t])){
#      
#      repeat{
#      
#      skip_to_next_1 <- FALSE
#      
#      trend_all_cat = tryCatch(gtrends(keyword= c('CDU', 'SPD', 'Grüne', 'Linke', 'FDP'), 
#                                       geo = "DE" , category = i , time = o , gprop = "web", onlyInterest = TRUE), error = function(e) { skip_to_next_1 <<- TRUE})
#      
#      if(skip_to_next_1 == TRUE) { next }     
#      Sys.sleep(5)
#      
#      skip_to_next_2 <- FALSE
#      
#      trend_all_cat_AFD = tryCatch(gtrends(keyword= c('CDU', 'SPD', 'Grüne', 'FDP', "Afd"), 
#                                           geo = "DE" , category = i, time = o , gprop = "web", onlyInterest = TRUE), error = function(e) { skip_to_next_2 <<- TRUE})
#      
#      if(skip_to_next_1 == FALSE & skip_to_next_2 == FALSE & is.null(trend_all_cat[[1]][1]) == FALSE & is.null(trend_all_cat_AFD[[1]][1]) == FALSE) { 
#        
#        break
#      }
#      
#      if(l == 5){
#        
#        
#        break
#        
#      }
#      
#      l = l + 1
#      print("Again")
#      
#      }   
#      
#      
#      Comp_categories$Data_2017[t] <- list(rbind(trend_all_cat$interest_over_time, 
#                                                 anti_join(trend_all_cat_AFD$interest_over_time, trend_all_cat$interest_over_time, by = "keyword")))
#      
#      
#      mem <- as.data.frame(Comp_categories$Data_2017[t]) %>%
#        group_by(keyword) %>%
#        mutate(hits = str_replace(hits, "<1", "0"), # HIER WEITER
#               hits = as.numeric(hits)) %>%
#        dplyr::summarise(mean_hits = mean(hits)) %>%
#        mutate(dev_hits = mean_hits - Comp_categories$Elec_results17[[t]][2],
#               gprop = ifelse(mean_hits >= 1, mean_hits/sum(mean_hits)*100, 0),
#               dev_gprop = gprop -  Comp_categories$Elec_results17[[t]][2],
#               sum_dev_hits_abs = sum(abs(dev_hits$share)),
#               sum_dev_prop_abs = sum(abs(dev_gprop$share)))
#      
#      Comp_categories$Dev17[t] <- list(mem)
#      
#      print(i)
#      print(t)
#     
#    }
#    
#    
#    if (o == "2013-01-01 2013-09-26" & is.na(Comp_categories$Data_2013[t])){
#      
#      repeat{
#      
#      skip_to_next_1 <- FALSE
#      
#      trend_all_cat = tryCatch(gtrends(keyword= c('CDU', 'SPD', 'Grüne', 'Linke', 'FDP'), 
#                                       geo = "DE" , category = i , time = o , gprop = "web", onlyInterest = TRUE), error = function(e) { skip_to_next_1 <<- TRUE})
#      
#      if(skip_to_next_1 == TRUE) { next }     
#      Sys.sleep(5)
#      
#      skip_to_next_2 <- FALSE
#      
#      trend_all_cat_AFD = tryCatch(gtrends(keyword= c('CDU', 'SPD', 'Grüne', 'FDP', "Afd"), 
#                                           geo = "DE" , category = i, time = o , gprop = "web", onlyInterest = TRUE), error = function(e) { skip_to_next_2 <<- TRUE})
#      
#      if(skip_to_next_1 == FALSE & skip_to_next_2 == FALSE & is.null(trend_all_cat[[1]][1]) == FALSE & is.null(trend_all_cat_AFD[[1]][1]) == FALSE) { 
#        
#        break
#      }
#      
#      if(l == 5){
#        
#        
#        break
#        
#      }
#      
#      l = l + 1
#      print("Again")
#      
#      }      
#      
#      Comp_categories$Data_2013[t] <- list(rbind(trend_all_cat$interest_over_time, 
#                                                 anti_join(trend_all_cat_AFD$interest_over_time, trend_all_cat$interest_over_time, by = "keyword")))
#      
#      
#      mem <- as.data.frame(Comp_categories$Data_2013[t]) %>%
#        group_by(keyword) %>%
#        mutate(hits = str_replace(hits, "<1", "0"), # HIER WEITER
#               hits = as.numeric(hits)) %>%
#        dplyr::summarise(mean_hits = mean(hits)) %>%
#        mutate(dev_hits = mean_hits - Comp_categories$Elec_results13[[t]][2],
#               gprop = ifelse(mean_hits >= 1, mean_hits/sum(mean_hits)*100, 0),
#               dev_gprop = gprop -  Comp_categories$Elec_results13[[t]][2],
#               sum_dev_hits_abs = sum(abs(dev_hits$share)),
#               sum_dev_prop_abs = sum(abs(dev_gprop$share)))
#      
#      Comp_categories$Dev13[t] <- list(mem)
#      
#      
#      print(i)
#      print(t)
#      
#    }
#    
#    
#    if (o == "2009-01-01 2009-09-26" & is.na(Comp_categories$Data_2009[t])){
#      
#      repeat{
#      
#      skip_to_next_1 <- FALSE
#      
#      trend_all_cat = tryCatch(gtrends(keyword= c('CDU', 'SPD', 'Grüne', 'Linke', 'FDP'), 
#                                       geo = "DE" , category = i , time = o , gprop = "web", onlyInterest = TRUE), error = function(e) { skip_to_next_1 <<- TRUE})
#      
#      if(skip_to_next_1 == TRUE) { next }     
#      Sys.sleep(5)
#      
#      if(skip_to_next_1 == FALSE & is.null(trend_all_cat[[1]][1]) == FALSE) { 
#        
#        break
#      }
#      
#      if(l == 5){
#        
#        
#        break
#        
#      }
#      
#      l = l + 1
#      print("Again")
#      
#      }
#      
#      Comp_categories$Data_2009[t] <- list(trend_all_cat$interest_over_time)
#      
#      
#      mem <- as.data.frame(Comp_categories$Data_2009[t]) %>%
#        group_by(keyword) %>%
#        mutate(hits = str_replace(hits, "<1", "0"), # HIER WEITER
#               hits = as.numeric(hits)) %>%
#        dplyr::summarise(mean_hits = mean(hits)) %>%
#        mutate(dev_hits = mean_hits - Comp_categories$Elec_results09[[t]][2],
#               gprop = ifelse(mean_hits >= 1, mean_hits/sum(mean_hits)*100, 0),
#               dev_gprop = gprop -  Comp_categories$Elec_results09[[t]][2],
#               sum_dev_hits_abs = sum(abs(dev_hits$share)),
#               sum_dev_prop_abs = sum(abs(dev_gprop$share)))
#      
#      Comp_categories$Dev09[t] <- list(mem)
#      
#      print(i)
#      print(t)
#      
#    }
#  }
#}
#
#
## Calculate the average absolute deviation of each category over all four election years
#for (k in 1:nrow(Comp_categories)){
#
#Comp_categories$Mean_dev_hits[k] <- mean(c(Comp_categories$Dev09[[k]][[6]][1], Comp_categories$Dev13[[k]][[6]][1],
#                                              Comp_categories$Dev17[[k]][[6]][1], Comp_categories$Dev21[[k]][[6]][1]))
#
#Comp_categories$Mean_dev_gprop[k] <- mean(c(Comp_categories$Dev09[[k]][[7]][1], Comp_categories$Dev13[[k]][[7]][1],
#                                         Comp_categories$Dev17[[k]][[7]][1], Comp_categories$Dev21[[k]][[7]][1]))
#
#}
#
#
## Create a formatted table to display the results
#library(htmlTable)
#
#Comp_categories %>%
#  arrange(Mean_dev_gprop) %>%
#  select(Category_ID, Category_Name ,Mean_dev_gprop) %>%
#  mutate(Mean_dev_gprop = round(Mean_dev_gprop, digits =  2)) %>%
#  htmlTable()
#
## Save environment
#filename = paste0("Selection_category_all_", gsub(":", "-", Sys.time()), ".RData",sep="")
#save.image(paste0("./Category_Selection/Samples", (filename)))



library(gtrendsR)
library(dplyr)
library(stringr)
library(tidyverse)
library(htmlTable)
library(forcats)



##### Run the following code if you want to bind all collected datasets to one and calculate the final results #####

setwd("./Category_Selection/Samples")


load("Selection_category_all_2023-02-01 16-28-15.RData")

# prep for loops
names <- dir("./")
names <- names[!names %in% "Selection_category_all_2023-02-01 16-28-15.RData"]
names2 <- c("Data_2021", "Data_2017", "Data_2013", "Data_2009")
names3 <- c("Dev09", "Dev13", "Dev17", "Dev21")


# needed to bind datasets from different datasets together via rbind
final_df <- Comp_categories


# bind the data columns and dev columns of all collected datasets together
for (u in names){
  
  load(u)
  
  for (g in names2){
    
    for (z in 1:nrow(Comp_categories)){
  
  final_df[[g]][[z]] <- rbind(data.frame(final_df[[g]][z]), data.frame(Comp_categories[[g]][z]))

    }
  }
  
  for (s in names3){
    
    for (z in 1:nrow(Comp_categories)){
  
      final_df[[s]][[z]] <- rbind(data.frame(final_df[[s]][z]), data.frame(Comp_categories[[s]][z]))
      
    }
  }
}
  
  
# create new empty columns for the following loop, in which the absolute average deviation from the election results and the respective ci.intervals for the respective election years are written
final_df <- cbind(final_df, Mean_Dev09 = NA, Mean_Dev13 = NA, Mean_Dev17 = NA, Mean_Dev21 = NA, 
                  Mean_gprop_lower.ci = NA, Mean_gprop_upper.ci = NA)



for (z in 1:nrow(Comp_categories)){
        

    mem <- final_df$Dev09[[z]] %>%
                group_by(keyword) %>%
                summarise(Mean_dev_hits = mean(sum_dev_hits_abs), SD_mean_dev_hits = sd(sum_dev_hits_abs),
                          Mean_dev_gprop = mean(sum_dev_prop_abs), SD_mean_dev_gprop = sd(sum_dev_prop_abs), 
                          .groups = "keep")  %>% 
                mutate(mean_hits_lower.ci = Mean_dev_hits - 1.96*(SD_mean_dev_hits/sqrt(n())),
                       mean_hits_upper.ci = Mean_dev_hits + 1.96*(SD_mean_dev_hits/sqrt(n())),
                       mean_gprop_lower.ci = Mean_dev_gprop - 1.96*(SD_mean_dev_gprop/sqrt(n())),
                       mean_gprop_upper.ci = Mean_dev_gprop + 1.96*(SD_mean_dev_gprop/sqrt(n())))
    
    final_df$Mean_Dev09[z] <- list(mem)
  
    
    mem <- final_df$Dev13[[z]] %>%
                group_by(keyword) %>%
                summarise(Mean_dev_hits = mean(sum_dev_hits_abs), SD_mean_dev_hits = sd(sum_dev_hits_abs),
                          Mean_dev_gprop = mean(sum_dev_prop_abs), SD_mean_dev_gprop = sd(sum_dev_prop_abs), 
                          .groups = "keep")  %>% 
                mutate(mean_hits_lower.ci = Mean_dev_hits - 1.96*(SD_mean_dev_hits/sqrt(n())),
                       mean_hits_upper.ci = Mean_dev_hits + 1.96*(SD_mean_dev_hits/sqrt(n())),
                       mean_gprop_lower.ci = Mean_dev_gprop - 1.96*(SD_mean_dev_gprop/sqrt(n())),
                       mean_gprop_upper.ci = Mean_dev_gprop + 1.96*(SD_mean_dev_gprop/sqrt(n())))
    
    final_df$Mean_Dev13[z] <- list(mem)
    
    
    mem <- final_df$Dev17[[z]] %>%
                group_by(keyword) %>%
                summarise(Mean_dev_hits = mean(sum_dev_hits_abs), SD_mean_dev_hits = sd(sum_dev_hits_abs),
                          Mean_dev_gprop = mean(sum_dev_prop_abs), SD_mean_dev_gprop = sd(sum_dev_prop_abs), 
                          .groups = "keep")  %>% 
                mutate(mean_hits_lower.ci = Mean_dev_hits - 1.96*(SD_mean_dev_hits/sqrt(n())),
                       mean_hits_upper.ci = Mean_dev_hits + 1.96*(SD_mean_dev_hits/sqrt(n())),
                       mean_gprop_lower.ci = Mean_dev_gprop - 1.96*(SD_mean_dev_gprop/sqrt(n())),
                       mean_gprop_upper.ci = Mean_dev_gprop + 1.96*(SD_mean_dev_gprop/sqrt(n())))
    
    final_df$Mean_Dev17[z] <- list(mem)
    
    
    mem <- final_df$Dev21[[z]] %>%
      group_by(keyword) %>%
      summarise(Mean_dev_hits = mean(sum_dev_hits_abs), SD_mean_dev_hits = sd(sum_dev_hits_abs),
                Mean_dev_gprop = mean(sum_dev_prop_abs), SD_mean_dev_gprop = sd(sum_dev_prop_abs), 
                .groups = "keep")  %>% 
      mutate(mean_hits_lower.ci = Mean_dev_hits - 1.96*(SD_mean_dev_hits/sqrt(n())),
             mean_hits_upper.ci = Mean_dev_hits + 1.96*(SD_mean_dev_hits/sqrt(n())),
             mean_gprop_lower.ci = Mean_dev_gprop - 1.96*(SD_mean_dev_gprop/sqrt(n())),
             mean_gprop_upper.ci = Mean_dev_gprop + 1.96*(SD_mean_dev_gprop/sqrt(n())))
    
    final_df$Mean_Dev21[z] <- list(mem)
      
}    


# Calculation of the absolute average deviation and the average of the ci.intervalls over all elections
for (z in 1:nrow(Comp_categories)){

  
        final_df$Mean_dev_hits[z] <-  mean(c(unique(final_df$Mean_Dev09[[z]][[2]]), 
                                           unique(final_df$Mean_Dev13[[z]][[2]]),
                                           unique(final_df$Mean_Dev17[[z]][[2]]), 
                                           unique(final_df$Mean_Dev21[[z]][[2]])))
        
        final_df$Mean_dev_gprop[z] <- mean(c(unique(final_df$Mean_Dev09[[z]][[4]]),
                                           unique(final_df$Mean_Dev13[[z]][[4]]),
                                           unique(final_df$Mean_Dev17[[z]][[4]]), 
                                           unique(final_df$Mean_Dev21[[z]][[4]])))
        
        final_df$Mean_gprop_lower.ci[z] <- mean(c(unique(final_df$Mean_Dev09[[z]][[8]]), 
                                                unique(final_df$Mean_Dev13[[z]][[8]]),
                                                unique(final_df$Mean_Dev17[[z]][[8]]), 
                                                unique(final_df$Mean_Dev21[[z]][[8]])))
        
        final_df$Mean_gprop_upper.ci[z] <- mean(c(unique(final_df$Mean_Dev09[[z]][[9]]),
                                                unique(final_df$Mean_Dev13[[z]][[9]]),
                                                unique(final_df$Mean_Dev17[[z]][[9]]), 
                                                unique(final_df$Mean_Dev21[[z]][[9]])))
        
      }



# reorder final dataset
final_df <- final_df %>%
  relocate(Mean_dev_hits, Mean_dev_gprop, .before = Mean_gprop_lower.ci)


# create table with final results containing the absolute average deviation and the average c.intervals over all four election years
final <- final_df %>%
  arrange(Mean_dev_gprop) %>%
  select(Category_ID, Category_Name ,Mean_dev_gprop, Mean_gprop_lower.ci, Mean_gprop_upper.ci) %>%
  mutate(Mean_dev_gprop = round(Mean_dev_gprop, digits =  2),
         Mean_gprop_lower.ci = round(Mean_gprop_lower.ci, digits = 2),
         Mean_gprop_upper.ci= round(Mean_gprop_upper.ci, digits = 2)) %>%
  htmlTable()



# Reorder the data for the plot
final_p <- final_df %>%
  select(Category_Name, Mean_dev_gprop, Mean_gprop_lower.ci, Mean_gprop_upper.ci) %>%
  arrange(Mean_dev_gprop) %>%
  mutate(Category_Name = as.factor(Category_Name))


p <- ggplot(final_p, aes(y = Mean_dev_gprop, x = fct_rev(fct_inorder(Category_Name)))) +
  scale_y_continuous(limits = c(0, 135), breaks = seq(0,135,5)) +
  geom_point(aes(x = fct_rev(fct_inorder(Category_Name)), 
                 y = Mean_dev_gprop),
             color = ifelse(final_p$Category_Name == "Law & Government", "green3", 
                            ifelse(final_p$Category_Name == "All categories", "orange",  "black")), size = 2) + 
  #geom_text(aes(label = round(Mean_dev_gprop, digits = 2)), hjust = -3) +
  geom_linerange(aes(x = Category_Name, 
                     ymin = Mean_gprop_lower.ci,
                     ymax = Mean_gprop_upper.ci),
                 color = ifelse(final_p$Category_Name == "Law & Government", "green3", 
                                ifelse(final_p$Category_Name == "All categories", "orange",  "black")),
                 lwd = 1) + 
  #ggtitle("Avg. absolute deviation of Gprop over all election years") +
  ylab("Avg. absolute percentage deviation of the Google Proportion from\nall four election years (Samples n = 10)") +
  xlab("Google Trends supercategories") +
  coord_flip() +
  theme_minimal(base_size = 22)


p + annotate(geom = "text", 
                  x = final_p$Category_Name[final_p$Category_Name == "Law & Government"], 
                  y = round(final_p$Mean_gprop_upper.ci[which(final_p$Category_Name == "Law & Government")], digits = 2) + 5,
                  label = paste(round(final_p$Mean_dev_gprop[which(final_p$Category_Name == "Law & Government")], digits =2), 
                                "(95% CI:",round(final_p$Mean_gprop_lower.ci[which(final_p$Category_Name == "Law & Government")], digits = 2), 
                                "/" , round(final_p$Mean_gprop_upper.ci[which(final_p$Category_Name == "Law & Government")], digits = 2), ")"),
                  color = "black", size = 6,
                  hjust = 0) +
  annotate(geom = "text", 
           x = final_p$Category_Name[final_p$Category_Name == "All categories"], 
           y = round(final_p$Mean_gprop_upper.ci[which(final_p$Category_Name == "All categories")], digits = 2) + 5,
           label = paste(round(final_p$Mean_dev_gprop[which(final_p$Category_Name == "All categories")], digits =2), 
                         "(95% CI:",round(final_p$Mean_gprop_lower.ci[which(final_p$Category_Name == "All categories")], digits = 2), 
                         "/" , round(final_p$Mean_gprop_upper.ci[which(final_p$Category_Name == "All categories")], digits = 2), ")"),
           color = "black", size = 6,
           hjust = 0)

setwd("..")
setwd("..")

```

@fig-A2 highlights how the search terms peak for the AfD, FDP, Grüne and SPD. For the period from 2004 - 2007, for the party "Die Linke" (see Figure 1) we found that the abbreviation "PDS" and the term "Linkspartei" are useful as the best indicators for predicting elections. The reason is that the party was still called "Party of Democratic Socialism" and bore the abbreviation "PDS" during those years.


```{r}
#| label: fig-A2
#| fig-cap: "Search terms for other parties"
#| fig-width: 14
#| fig-height: 17
#| fig-pos: "H"


# CDU
#result: CDU + Angela Merkel + CSU
#müssen hier nochmal ziehen für Armin Laschet, bekomme momentan nur Fehlercode
#  CSU
#result: Just use CSU, no Peaks for main candidates
# SPD 
#Kanzlerkandidaten: Gerhard Schröder (2005), Frank-Walter Steinmeier (2009), Peer Steinbrück(2013), Martin Schulz(2017)
#result: SPD + Candidates
# FDP
#Parteivorsitzende: Guido Westerwelle (-2011), Philipp Rösler (2011-2013), Christian Linder (2013-)
#result: FDP + Candidate
# Die Grünen
#top candidates: Joschka Fischer(2005), Jürgen Trittin + Renate Künast (2009), Jürgen Trittin + Katrin Göring Eckardt (2013), Katrin Göring-Eckardt + Cem Özdemir (2017)
#result: Grüne (includes "Die Grünen" and "Bündnis 90 die Grünen") + top candidates
# Die Linke
#Spitzenkandidaten: 2005: PDS, Gregor Gysi ,Gregor Gysi (2009)  ,Gregor Gysi + sarah Wagenknecht + (Dietmar Bartsch) + (5 weitere) (20013) ,Dietmar Bartsch + Sarah Wagenknecht (2017)
#result: Linke (includes "Die Linke" and "Die Linken")
#Linke vs related querries
#Spitzenkandidaten
#for election 2013: Sarah Wagenknecht and Gregor Gysi (Most important ones of the 8 candidates due to Google Trends)
#Overall 2005
#PDS + Linkspartei
# AFD
#Spitzenkandidaten:  Bernd Lucke (2013),Alice Weidel + Alexander Gauland (2017),Alice Weidel + Tino Chrupalla (2021)
#result: AFD + candidates

# Collect data ####
    # terms_CDU <- gtrends(keyword=c("CDU", "CSU", "Angela Merkel", "Christlich demokratische Union","Armin Laschet"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
    # terms_CSU <- gtrends(keyword=c("CSU", "christlich soziale union", "Edmund Stoiber", "Horst Seehofer", "Alexander Dobrindt"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
    # terms_SPD <- gtrends(keyword=c("SPD", "Sozialdemokratische Partei Deutschlands"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
    # terms_SPD_Cand <- gtrends(keyword=c("SPD", "Frank Walter Steinmeier", "Peer Steinbrück", "Martin Schulz", "Olaf Scholz"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
    # terms_FDP <- gtrends(keyword=c("FDP", "Freie demokratische Partei Deutschlands", "Guido Westerwelle", "Philipp Rösler", "Christian Lindner"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
    # terms_Grüne <- gtrends(keyword=c('"Grüne"',"Die Grünen", "Bündnis 90 die Grünen"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
    # terms_Grüne_Cand <- gtrends(keyword=c("Grüne","Jürgen Trittin + Renate Künast", "Jürgen Trittin + Katrin Göring Eckardt", "Katrin Göring Eckardt + Cem Özdemir", "Baerbock"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
    # terms_Linke <- gtrends(keyword=c("Linke", "Die Linke","Die Linken"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
    # terms_Linke_Cand <- gtrends(keyword=c("Gregor Gysi", "Gregor Gysi + Sarah Wagenknecht", "Dietmar Bartsch + Sarah Wagenknecht", "Janine Wissler + Dietmar Bartsch"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")
    # terms_Linke2 <- gtrends(keyword=c("Die Linke", "Die Linke PDS","PDS", "Linke", "Linkspartei"), geo= "DE" , category=19, time = "2004-01-01 2005-09-24", gprop="web")
    # terms_AFD <- gtrends(keyword=c("AFD", "Alternative für Deutschland", "Bernd Lucke", "Alice Weidel + Alexander Gauland", "Alice Widel + Tino Chrupalla"), geo= "DE" , category=19, time = "2004-01-01 2021-12-31", gprop="web")



# Save relevant datasets for reproducibility ####
  # save(terms_SPD, file = paste0("terms_SPD_", gsub(":|\\s", "_", Sys.time()), ".RData"))
  # save(terms_Grüne, file = paste0("terms_Grüne_", gsub(":|\\s", "_", Sys.time()), ".RData"))
  # save(terms_AFD, file = paste0("terms_AFD_", gsub(":|\\s", "_", Sys.time()), ".RData"))
  # save(terms_FDP, file = paste0("terms_FDP_", gsub(":|\\s", "_", Sys.time()), ".RData"))

# Clean environment
# rm(list=ls())

# Load data
load(file = "terms_SPD_2023-02-27_11_02_50.RData")
load(file = "terms_Grüne_2023-02-27_11_04_41.RData")
load(file = "terms_AFD_2023-02-27_11_05_16.RData")
load(file = "terms_FDP_2023-02-27_11_06_00.RData")


# Modify data

for(i in c("terms_AFD", "terms_FDP", "terms_Grüne", "terms_SPD")){
  data_i <- get(i)$interest_over_time%>%
    mutate(hits = as.numeric(hits), date = as.Date(date)) %>%
    replace(is.na(.), 0)
  assign(paste0(i, "_df"), data_i)
  
}



# Generate plots ####
elec_vlines <- as.Date(c("2009-09-27", "2013-09-18", "2017-09-24", "2021-09-26")) # "2005-09-18", 



for(i in c("terms_AFD_df", "terms_FDP_df", "terms_Grüne_df", "terms_SPD_df")){
  assign(paste0("plot_", i),
  get(i) %>% ggplot(aes(x=date, y=hits, group=keyword, col=keyword)) + 
    geom_line(size=1)  +
    scale_y_continuous(breaks = seq(0,100, 10)) +
    geom_vline(xintercept = elec_vlines, col= "black", linetype="dotted", size = 1) +
    theme_minimal(base_size = 22) +
    ylab("Searches (100 = max. interest in time period/territory)") +
    xlab("Date") +
    labs(colour = "Search terms (below)") +
    scale_x_date(date_labels = paste0("%y", "'"),
                 date_breaks = "1 year",
                 limits = as.Date(c("2006-01-01", "2021-12-01"))) +
    theme(legend.position = "top",
          axis.title.x=element_blank(),
          axis.title.y=element_blank()) +
    annotate(geom = "text", 
             label = paste0("Election: ", elec_vlines), 
             x=elec_vlines+60, 
             y=rep(100, length(elec_vlines)),
             angle = 90,
             hjust = 1,
             size = 5)+
    guides(col=guide_legend(nrow=2,byrow=TRUE)))

}



result <- plot_terms_AFD_df + plot_terms_FDP_df + plot_terms_Grüne_df + plot_terms_SPD_df + 
  plot_layout(ncol = 1)
result <- patchwork::patchworkGrob(result)

gridExtra::grid.arrange(result,
                                            bottom=grid::textGrob("Date", gp=grid::gpar(fontsize=22)), 
                                            left=grid::textGrob("Searches (100 = max. interest in time period/territory)", gp=grid::gpar(fontsize=22), rot=90))




# Overview over search terms
######### Search terms + period
#Bundestagswahlen: 26.09.2021; 24.09.2017; 22.09.2013; 27.09.2009; 18.09.2005
# Specifying vertical X intercepts for election dates
###############################  CSU #############################
#result: Just use CSU, no Peaks for main candidates
###############################  SPD #############################
#Kanzlerkandidaten: Gerhard Schröder (2005), Frank-Walter Steinmeier (2009), Peer Steinbrück(2013), Martin Schulz(2017)
#result: SPD + Candidates
#SPD Candidates
###############################  FDP #############################
#Parteivorsitzende: Guido Westerwelle (-2011), Philipp R?sler (2011-2013), Christian Linder (2013-)
#result: FDP + Candidate
###############################  Die Grünen #############################
#top candidates: Joschka Fischer(2005), Jürgen Trittin + Renate Künast (2009), Jürgen Trittin + Katrin Göring Eckardt (2013), Katrin Göring-Eckardt + Cem Özdemir (2017)
#result: Grüne (includes "Die Grünen" and "Bündnis 90 die Grünen") + top candidates
###############################  Die Linke #############################
#Spitzenkandidaten: 2005: PDS, Gregor Gysi ,Gregor Gysi (2009)  ,Gregor Gysi + sarah Wagenknecht + (Dietmar Bartsch) + (5 weitere) (20013) ,Dietmar Bartsch + Sarah Wagenknecht (2017)
#result: Linke (includes "Die Linke" and "Die Linken")
###Used in main Paper (Figure2)
#Candidates
#for election 2013: Sarah Wagenknecht and Gregor Gysi (Most important ones of the 8 candidates due to Google Trends)
#Overall 2005
#PDS + Linkspartei



```

## GT data collection and sampling error

Table A1 shows the collected datasets across a day that were different. We collected the data hourly, but Table A1 only shows the collected samples that are different. Each .Rdata file correspondings to a time point when we collected the respective GT datasets for the different search terms. In principle, we could have stored the single datasets contained in each .Rdata file as .csv-files. But the approach we chose turned out to be more feasable. As discussed by Raubenheimer et al. [@raubenheimer_hey_2021] google trends data samples may differ across time. Across the series of datasets we collected we checked for each .RData file if all the contained datasets are different as compared to the previously collected samples. Subsequently, we only used samples for which we identified that they are not equivalent to the previous ones. For replication purposes the corresponding .RData files are highlighted in the column "used" in Table A1. The end time of the respective data collection is indicated in the "Time" column. The corresponding data collection took around 2 minutes.


```{r}
#| label: tbl-A1
#| tbl-cap: "Overview datasets across samples that are different"
#| tbl-colwidths: auto


  data_GT_data_set_comparisons <- read_csv("data_GT_data_set_comparisons.csv")

 data_GT_datasets_good <- read_csv("data_GT_datasets_good.csv")

datasets_table <- data_GT_data_set_comparisons %>% 
  select(name, check, no_dataset_identical, how_many_dataset_identical) %>% 
  filter(no_dataset_identical==TRUE)
# GOOD = ALL datasets in .Rdata in this row are different from the .Rdata in the row before

# Filter out one RDate per day

datasets_table$date <- as.Date(str_extract(datasets_table$name, 
                                          pattern = "^[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]"))

datasets_table$time <- sub(".+? ", "", datasets_table$name)
datasets_table$time <- sub(".RData", "", datasets_table$time)
datasets_table$used <- ifelse(datasets_table$name %in% data_GT_datasets_good$name, "used", "-")

datasets_table %>%
  select(name, date, time, used) %>%
  kable(col.names = c("Dataset name", "Date",
                      "Time", "Used"), booktabs = TRUE) %>%
  kable_styling(font_size = 6)

```



@fig-A3 visualizes exemplarily the variation of the Google proportion across 10 different samples for the 2021 election. In order to visualize this variation we used the same samples as used for the prediction of the 2021 election. The Google proportion was calculated for each day for each party in each sample. Subsequently, the mean and confidence intervals were calculated for each day for each party. @fig-A3 displays the mean (solid line) and confidence intervals (shaded area) across days and parties. We find that the variability accross samples is relatively low.


```{r}
#| label: fig-A3
#| fig-cap: "Sampling error variation for the 2021 election"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"


### Function: Replace search terms ####
replace_searchterms <- function(x){
  
  # CDU
  x <- gsub('CDU.*', 'CDU', x)
  
  # SPD
  x <- gsub('SPD.*', 'SPD', x)
  
  # GREENS
  x <- gsub('Jürgen.*', 'Grüne', x)
  x <- gsub('Annalena.*', 'Grüne', x)
  x <- gsub('Cem.*', 'Grüne', x)
  x <- gsub('Katrin.*', 'Grüne', x)
  x <- gsub('Grüne.*', 'Grüne', x)
  
  
  
  # LINKE
  x <- gsub('Linke.*', 'Linke',  x)
  x <- gsub('PDS.*', 'Linke',  x)
  
  # FDP
  x <- gsub('FDP.*', 'FDP', x)
  
  # AFD
  x <- gsub('Bernd.*', 'AFD', x)
  x <- gsub('Alice.*', 'AFD', x)
  
  #Sontige
  x <- gsub('NPD.*', 'Sonstige', x)
  x <- gsub('REP.*', 'Sonstige', x)
  x <- gsub('Freie.*', 'Sonstige', x)
}


# Datasets with category ####
#all Datsets
setwd("./Data")
df <- list.files(full.names = FALSE)

years <- c("_05", "_09", "_13", "_17", "_21")
years2 <- c("2005", "2009", "2013", "2017", "2021")




for (y in 1:length(df)){ # 
  #y <- 1

  load(df[y])
  
  cnt <- 0
  
  for (t in years){
  
  objects <- ls()[str_detect(ls(), t)]
  objects <- objects[!str_detect(objects, "Figure")]
  
  length <- 1:length(objects)
  
  cnt <- cnt +1
  
  mem <- data.frame()
    
  # checks if a dataset for 2009 is loaded (only two objects in it, because AfD did not exist yet)
  if(max(length) == 2){
    
  # bind objects together "trend_CDU_09" and "trend_sonst_09" with anti_join together -> joins only the part in "trend_sonst_09" that is different from "trend_CDU_09"  
  mem <- rbind(get(objects[1])$interest_over_time, 
                          anti_join(get(objects[2])$interest_over_time, get(objects[1])$interest_over_time, by = "keyword"))
  
  mem <- mem %>%
    mutate(hits = as.numeric(hits), date = as.Date(date), keyword = replace_searchterms(keyword)) %>%
    replace(is.na(.), 0) %>%
    select(date, hits, keyword)
  
  assign(paste0("df", years2[cnt], "_", filename), mem)
  
  }
  
  # checks if a dataset for 2013/17/21 is loaded
  if(max(length) > 2){
    
    mem <- rbind(get(objects[1])$interest_over_time, 
                     anti_join(get(objects[2])$interest_over_time, get(objects[1])$interest_over_time, by = "keyword"))
    
    mem <- rbind(mem, anti_join(get(objects[3])$interest_over_time, get(objects[1])$interest_over_time, by = "keyword"))
    
    mem <- mem %>%
      mutate(hits = as.numeric(hits), date = as.Date(date), keyword = replace_searchterms(keyword)) %>%
      replace(is.na(.), 0) %>%
      select(date, hits, keyword)
    
    assign(paste0("df", years2[cnt], "_", filename), mem)
    
    
    
    }
  }
}
  
  

# pick all datasets for 2021
names <- ls()[str_detect(ls(), "df2021")]

mem2 <- rbind(get(names[1]), get(names[2]), get(names[3]), get(names[4]), get(names[5]),
      get(names[6]), get(names[7]), get(names[8]), get(names[9]), get(names[10]))


## compute mean, sd, ci intervals over ten datasets for hits and Gprop
plot21 <- mem2 %>%
  rename(party=keyword) %>%
  group_by(date, party) %>%
  summarize(Mean_hits = mean(hits), hits_sum = sum(hits), SD = sd(hits)) %>% # Same as before but diff. code
  mutate(Gprop = ifelse(hits_sum >= 1, hits_sum/sum(hits_sum)*100, 0)) %>%
  mutate(Mean_hits_lower.ci = Mean_hits - 1.96*(SD/sqrt(n())),
         Mean_hits_upper.ci = Mean_hits + 1.96*(SD/sqrt(n())),
         Gprop_lower.ci =  Gprop - 1.96*(SD/sqrt(n())),
         Gprop_upper.ci =  Gprop + 1.96*(SD/sqrt(n()))) %>%
  mutate(party = as.factor(party)) %>%
  filter(party != "Sonstige")
#
#
## hits (grid)
#grid_hits <-  plot21 %>%
#            filter(date > "2021-08-01") %>%
#            ggplot(aes(x = date, y = Mean_hits, group = party, color = party)) +
#            geom_line(size = 1) +
#            geom_ribbon(aes(ymin = Mean_hits_lower.ci, ymax = Mean_hits_upper.ci,  fill = party), alpha = 0.4) +
#            scale_color_manual(values = c("AFD" = "deepskyblue1", "CDU" = "black", "FDP" = "yellow","Grüne" = "green3", "Linke" = "purple", "SPD" = "red")) +
#            facet_grid(facets = vars(party), scales = "free") + #coord_cartesian(ylim = c(0,25)) +
#            ggtitle("Google Trends Sample Error 2021") +
#            labs(y = "Mean of hits")
#
#grid_hits

# gprop (grid)
grid_gprop <- plot21 %>%
  filter(date > "2021-08-01") %>%
  ggplot(aes(x = date, y = Gprop, group = party, color = party)) +
  geom_line(size = 0.5) +
  geom_ribbon(aes(ymin = Gprop_lower.ci, ymax = Gprop_upper.ci,  fill = party), 
              alpha = 0.1,
              linetype = "dashed") +
  scale_color_manual(values = c("AFD" = "blue", "CDU" = "black", "FDP" = "orange","Grüne" = "green", "Linke" = "purple", "SPD" = "red")) +
  facet_grid(facets = vars(party)) + # , scales = "free"coord_cartesian(ylim = c(0,25)) +
  labs(y = "Mean of Google Proportion over samples and 95% CI", x = "Time span", color = "Party", fill= "Party") + 
  theme_minimal(base_size = 22) +
  theme(legend.position="top", 
        legend.direction = "horizontal",
        panel.spacing = unit(2, "lines"))+
  guides(color = guide_legend(nrow = 1)) + 
  scale_x_date(date_breaks = "weeks" , date_labels = "%b %d")




grid_gprop


setwd("..")


```



## Models and weighting methods

Challenges when calculating the weighting factor M3: In single cases, especially for small intervals, it can occur that the mean Google proportion for single parties within an interval is 0 when calculating the weighting factor. If the poll for weighting were then divided by the mean Google proportion, the weighting factor for that party would be 0 and the data to be weighted would also be 0. To circumvent this problem, the 0 of this party in the Google proportion is replaced by the value the party has in the poll used for weighting and when this poll is divided by the Google proportion to calculate the weighting factor, the weighting factor for this party is not 0 but 1, which means that no weighting takes place for this party and solely the Google Proportion of this party is used. A further issue that had to be circumvented is that the 2013 polls do not consistently include a value for the AFD party, as this party was newly founded in 2013. The procedure is therefore to ignore the calculation of the weighting factor and assign a weighting value of 1 in these cases to the AFD, so that no weighting takes place if the entry is missing and only the Google Proportion is used.

Challenges when applying the weighting factor M3: When applying the calculated weighting factors, the same problem as previously mentioned can occur, i.e. that single parties in the mean Google proportion, on which the weighting factor has to be applied, have the value 0. To circumvent this issue, we assign this party the value 1 so that the respective party can be weighted. If previously in the calculation of the weighting factor the mean Google proportion of a party was 0 or the AFD was not included in the 2013 polls and accordingly the value 0 of the weighting factor was replaced by 1 and the mean Google proportion of a party to which this weighting factor of 1 is to be applied is also 0, then the corresponding poll values of the poll used for calculating the weighting factor for these parties are to be inserted, since the Google data cannot be used and our model would be too heavily biased as a result.

## Further results

### Comparing predictions across GT data windows, across parties: Other elections and widths

@fig-A4 is the same as @fig-4 only that we added additional data windows that vary in terms of width (21, 42, 56, 70 days). Generally, our conclusions from @fig-4 do not change once we add additional intervals in @fig-A1. @fig-A2 provides results for the other three elections (2009, 2013, 2017). @fig-A2 highlights that there is considerable differences across elections.


```{r}
#| label: fig-A4
#| fig-cap: "Accuracy of GT predictions for different parties and party shares across data windows"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"



## Figure A1 (Figure 2 for appendix) ####
data_plot1 <- data_plot %>%
  filter(datasource_weight =="GT") %>%
  group_by(model_name) %>%
  mutate(group_mean_deviation = mean(abs(Mean_dev))) %>%
  filter(election_date == "2021-09-26")

# Create x-axis tick labels
x_tick_labels <- data_plot1 %>%
  group_by(GT_end_date) %>%
  filter(row_number()==1) %>%
  ungroup() %>%
  slice(c(1,25, 50, 75, 100, 125, 150)) %>%# Pick every 30th row
  select(GT_start_date, GT_end_date, model_time_distance)
  


ggplot(data_plot1,
            aes(x = GT_end_date,
                y = Mean_dev,
                color = party)) +
  geom_vline(xintercept = as.Date("2021-09-26"),
             linetype="dashed") +
  geom_hline(yintercept = 0,
             linetype="solid") +  
  geom_point(size = 0.3) +
  geom_line() +
  theme_minimal(base_size = 22) +
  facet_grid(vars(model_time_interval_fac),
             #vars(election_date), 
             scales = "free_x") +
  #facet_wrap(~model_time_interval_fac, ncol = 1) +
  # xlim(min(data_plot$GT_end_date) - 1, as.Date("2021-09-26")+1) +
  scale_x_date(breaks = x_tick_labels$GT_end_date,
               labels = paste0(x_tick_labels$model_time_distance, " day(s)\n[", x_tick_labels$GT_end_date, "]")
  ) +
  scale_y_continuous(sec.axis = dup_axis(
    name = "Width of data window")) +
  scale_color_manual(values = cols) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position="top",
        axis.text.y.right = element_blank()) +
  ylab("Deviation on % scale\n(prediction error)") +
  xlab("Distance of data window [end date of window]") +
  labs(colour = "Party")
# stat_summary(aes(y = group_mean_deviation, group = 1), fun=mean, colour="purple", geom="line", linetype = "dashed")
#+ geom_errorbar(aes(ymin=dev_lower.ci, ymax=dev_upper.ci),width=.3, position=position_dodge(.9))


# p
# ggsave(plot = p,
#        filename = "Figure_A1_2_all_intervalls.png", # e.g. change to pdf
#        width = 14,
#        height = 14,
#        device = "png", # e.g. change to pdf
#        dpi = 600)  

```



```{r}
#| label: fig-A5
#| fig-cap: "Accuracy of GT predictions for different parties and party shares across data windows"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"


data_plot2 <- data_plot %>%
  filter(datasource_weight =="GT") %>%
  group_by(model_name) %>%
  mutate(group_mean_deviation = mean(abs(Mean_dev))) %>%
  filter(election_date != "2021-09-26")

# Create x-axis tick labels

x_tick_labels <- data_plot2 %>%
  group_by(election_date, GT_end_date) %>%
  filter(row_number()==1) %>%
  group_by(election_date) %>%
  filter(model_time_distance == 1|
           model_time_distance == 50|
           model_time_distance == 100|
           model_time_distance == 150) %>%
  #slice(c(1,50,  100, 150)) %>%# Pick every 30th row
  ungroup() %>%
  select(GT_start_date, GT_end_date, model_time_distance) %>%
  arrange(GT_end_date, model_time_distance)



ggplot(data_plot2,
            aes(x = GT_end_date,
                y = Mean_dev,
                color = party)) +
  geom_vline(xintercept = as.Date("2017-09-24"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2013-09-22"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2009-09-27"),
             linetype="dashed") +
  geom_hline(yintercept = 0,
             linetype="solid") +  
  geom_point(size = 0.1) +
  geom_line(size = 0.2) +
  theme_minimal(base_size = 22) +
  facet_grid(vars(model_time_interval_fac),
             vars(election_date), 
             scales = "free_x") +
  scale_x_date(breaks = x_tick_labels$GT_end_date,
               labels = paste0(x_tick_labels$model_time_distance, " day(s)\n[", x_tick_labels$GT_end_date, "]")
  ) +
  scale_y_continuous(sec.axis = dup_axis(
    name = "Width of data window")) +
  scale_color_manual(values = cols) + 
  theme(axis.text.x = element_text(angle = 55, 
                                   hjust = 1,
                                   size= 14),
        legend.position="top",
        axis.text.y.right = element_blank()) +
  ylab("Deviation on % scale\n(prediction error)") +
  xlab("Distance of data window [end date of window]") +
  labs(colour = "Party")
# stat_summary(aes(y = group_mean_deviation, group = 1), fun=mean, colour="purple", geom="line", linetype = "dashed")
#+ geom_errorbar(aes(ymin=dev_lower.ci, ymax=dev_upper.ci),width=.3, position=position_dodge(.9))


# ggsave(plot = p,
#        filename = "Figure_A2_figure2_appendix.png", # e.g. change to pdf
#        width = 14,
#        height = 14,
#        device = "png", # e.g. change to pdf
#        dpi = 600)  


## Figure 4-table: Party variation ####
# Summarize across parties (variation across models)
# Table basically shows for which models + parties there is the highest variation
# of predictions across difference distances
data_plot1_party <- data_plot1 %>% 
  group_by(party, model_time_interval) %>%
  select(party, model_time_interval, Mean_dev) %>%
  summarize(Mean_dev_party = mean(abs(Mean_dev)), # What should we use here? absolute?
            Mean_dev_party_sd = sd(Mean_dev),
            Mean_dev_party_max = max(Mean_dev), 
            Mean_dev_party_min = min(Mean_dev)) %>%
  arrange(desc(Mean_dev_party_sd), party)


```

@tbl-A2 models the prediction accuracy as a function of the data windows distance to the election. Thereby, the width of the data window is held constant at 14 days.


```{r}
#| label: tbl-A2
#| tbl-cap: "Prediction accuracy as a function of distance days"
#| results: "asis"

# First model distance
data_modelling <- data_plot %>%
  filter(datasource_weight =="GT") %>%
  mutate(Mean_dev_absolute = abs(Mean_dev),
         model_time_distance_num = abs(as.numeric(model_time_distance)-150))  %>%
  filter(model_time_interval_fac == "14 days")

# model_time_distance_num: Distance reversed so that higher values are lower distance!
# Check: View(data_plot1 %>% select(model_time_distance, model_time_distance_num))# 
# View(data_plot1 %>% select(model_time_interval_fac, model_time_interval_fac_num))

M1 <- lm(Mean_dev_absolute ~ model_time_distance_num, data = data_modelling)
M2 <- lm(Mean_dev_absolute ~ model_time_distance_num, data = data_modelling %>% 
           filter(election_date=="2009-09-27"))
M3 <- lm(Mean_dev_absolute ~ model_time_distance_num, data = data_modelling %>% 
           filter(election_date=="2013-09-22"))
M4 <- lm(Mean_dev_absolute ~ model_time_distance_num, data = data_modelling %>% 
           filter(election_date=="2017-09-24"))
M5 <- lm(Mean_dev_absolute ~ model_time_distance_num, data = data_modelling %>% 
           filter(election_date=="2021-09-26"))


stargazer(M1, 
          M2,
          M3,
          M4, 
          M5,
          dep.var.caption = 'Dependent variable: Average absolute deviations',
          column.labels=c('\\shortstack{M1 \\\\ (all elections)}',
                          '\\shortstack{M2 \\\\ (2009-09-27)}',
                          '\\shortstack{M3 \\\\ (2013-09-22)}',
                          '\\shortstack{M4 \\\\ (2017-09-24)}',
                          '\\shortstack{M5 \\\\ (2021-09-26)}'),
          omit.stat=c("LL","ser","f"),
          dep.var.labels.include = FALSE,
          model.names = FALSE,
          font.size = "scriptsize",
          covariate.labels = "Distance (days)",
          header = F, 
          model.numbers=FALSE)


```

### Comparing predictions across model classes: Other elections and data window widths {#sec-comparing-model-classes-other}


```{r}
#| label: fig-A6
#| fig-cap: "Accuracy of predictions across model classes for 2009 election"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"

Figure_5_2009_09_27

```



```{r}
#| label: fig-A7
#| fig-cap: "Accuracy of predictions across model classes for 2013 election"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"

Figure_5_2013_09_22

```



```{r}
#| label: fig-A8
#| fig-cap: "Accuracy of predictions across model classes for 2017 election"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"

Figure_5_2017_09_24

```



## Replicating analysis using a "other parties" category for GT data

```{r data-import-sonstige}

# LOAD DATA (Sonstige) ####
# Remove all files
# rm(list=ls())
# ### For normal Data
# load(file  = "./Environments/Env_Merged_syntax_Sonstige 2023-01-26_01-11-02.RData")
# 
# # Delete unneccessary objects
# rm(list=setdiff(ls(), c("data_models", "data_predictions_final", "data_predictions"))) 
# 
# 
# ## Data management ####
# data_models <- data_models %>% 
#   filter(grepl("M_\\d+_2005", data_models$model_name) == FALSE)
# 
# #And again renaming since first id´s are missing (deleted 2005ÃÂ´s)
# ## Add model index/number
# data_models <- data_models %>% 
#   mutate(model_id = row_number()) %>%
#   select(model_id, everything())
# 
# # Reorder
# data_models <- data_models %>%
#   select(model_id, model_name, everything())
# 
# # Reorder
# data_models <- data_models %>%
#   select(model_id, model_name, everything())
# 
# 
# #### delete 2005 rows (just needed for Model2_2009)
# 
# data_predictions_final <- data_predictions_final %>% 
#   filter(grepl("M_\\d+_2005", data_predictions_final$model_name) == FALSE)
# 
# #And again renaming since first id´s are missing (deleted 2005ÃÂ´s)
# ## Add model index/number
# data_predictions_final <- data_predictions_final %>% 
#   mutate(model_id = row_number()) %>%
#   select(model_id, everything())
# 
# 
# 
# # CLEAN data_predictions
# data_predictions <- data_predictions %>%
#   filter(grepl("M_\\d+_2005", model_name) == FALSE) %>% # filter out models without predictions
#   group_by(model_name) %>%
#   mutate(model_id = cur_group_id()) %>% # Add new model_id (after filtering)
#   ungroup()

```




```{r}
#| label: fig-A9
#| fig-cap: "Figure 4 with 'other parties'"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"



# ## Prepare data for graph #
# data_predictions$party <-
#   as.factor(ordered(data_predictions$party,
#                     levels = c("Grüne", "Linke", "FDP", "AFD", "SPD", "CDU", "Sonstige"))) # ADAPTED
# data_predictions$deviation_label <-
#   round(data_predictions$deviation, 1)
# 
# #for Confidence Intervals
# 
# data_predictions_final_mean <- data_predictions_final %>%
#   group_by( model_name, party) %>%
#   summarise(Mean_dev = mean(deviation), SD_dev = sd(deviation), Mean = mean(prediction), SD = sd(prediction), 
#             .groups = "keep") %>%
#   mutate(mean_lower.ci = Mean - 1.96*(SD/sqrt(n())),
#          mean_upper.ci = Mean + 1.96*(SD/sqrt(n())),
#          dev_lower.ci = Mean_dev - 1.96*(SD_dev/sqrt(n())),
#          dev_upper.ci = Mean_dev + 1.96*(SD_dev/sqrt(n()))) #%>%
# # replace_na(.,0)
# 
# # Kann man nuch besser lösen ????????
# #Sinn: summarize die oben genannten aber behalte andere Spalten wie datasource_weight etc.
# data_predictions_final_mean <- merge(data_predictions_final_mean, data_predictions, by = c("model_name","party"))
# data_predictions_final_mean <- data_predictions_final_mean %>% select(-c(20,21,22,23,24), -("df_id"))




### Data for plots #
data_plot <- data_predictions_final_mean %>%
  #filter(election_date=="2017-09-24"|election_date=="2013-09-22") %>% #can filter for better overview
  mutate(model_time_interval_fac = factor(as.numeric(model_time_interval, "days"))) %>% # convert to days
  mutate(model_time_interval_fac = paste(model_time_interval_fac, " days", sep="")) %>%
  mutate(model_time_distance = election_date - GT_end_date) %>%
  mutate(model_time_interval_fac = factor(model_time_interval_fac,
                                          levels = c("7 days", "14 days", "21 days", 
                                                     "28 days", "42 days", "56 days", 
                                                     "70 days", "77 days", "84 days", "91 days"),
                                          ordered = TRUE))


cols <- c("SPD" = "red", 
          "CDU" = "black", 
          "AFD" = "blue", 
          "FDP" = "orange", 
          "Linke" = "purple", 
          "Grüne" = "green",
          "Sonstige" = "gray")  # ADAPTED


linetypes <- c("SPD" = "solid", 
               "CDU" = "solid", 
               "AFD" = "solid", 
               "FDP" = "solid", 
               "Linke" = "solid", 
               "Grüne" = "solid",
               "Sonstige" = "solid")  # ADAPTED




data_plot1 <- data_plot %>%
  filter(datasource_weight =="GT") %>%
  group_by(model_name) %>%
  mutate(group_mean_deviation = mean(abs(Mean_dev))) %>%
  filter(election_date == "2021-09-26") %>%
  filter(model_time_interval_fac == "7 days" |
           model_time_interval_fac == "14 days" |
           model_time_interval_fac == "28 days" |
           model_time_interval_fac == "91 days") %>% 
  ungroup() %>%
  mutate(model_time_interval_fac = recode(model_time_interval_fac,
                                          "7 days" = "Plot 1:\n7 days",
                                          "14 days" = "Plot 2:\n14 days",
                                          "28 days" = "Plot 3:\n28 days",
                                          "91 days" = "Plot 4:\n91 days"))



# Create x-axis tick labels
x_tick_labels <- data_plot1 %>%
  group_by(GT_end_date) %>%
  filter(row_number()==1) %>%
  ungroup() %>%
  slice(c(1,25, 50, 75, 100, 125, 150)) %>%# Pick every 30th row
  select(GT_start_date, GT_end_date, model_time_distance)



ggplot(data_plot1,
            aes(x = GT_end_date,
                y = Mean_dev,
                color = party#,
                #linetype = party
            )) +
  geom_vline(xintercept = as.Date("2021-09-26"),
             linetype="dashed") +
  geom_hline(yintercept = 0,
             linetype="solid",
             color = "lightgray") +  
  geom_point(size = 0.5) +
  geom_line() +
  theme_minimal(base_size = 22) +
  facet_grid(vars(model_time_interval_fac),
             scales = "free_x") +
  scale_x_date(breaks = x_tick_labels$GT_end_date,
               labels = paste0(x_tick_labels$model_time_distance, " day(s)\n[", x_tick_labels$GT_end_date, "]")
  ) +
  scale_y_continuous(sec.axis = dup_axis(
    name = "Width of data window")) +
  scale_color_manual(values = cols) + 
  scale_linetype_manual(values = linetypes) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position="top",
        axis.text.y.right = element_blank(),
        strip.background =element_rect(fill="white")) +
  ylab("Deviation on % scale\n(prediction error)") +
  xlab("Distance of data window [end date of window]") +
  labs(colour = "Party")
# stat_summary(aes(y = group_mean_deviation, group = 1), fun=mean, colour="purple", geom="line", linetype = "dashed")
#+ geom_errorbar(aes(ymin=dev_lower.ci, ymax=dev_upper.ci),width=.3, position=position_dodge(.9))


```



```{r}
#| label: fig-A10
#| fig-cap: "Figure 5 with 'other parties'"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"



# prediction error averaged across all parties for the GT data + other datasources

# Create average prediction error (across all parties) ###
###Plot GT vs. Polls
for(i in as.character(unique(data_plot$election_date))){ # Loop over elections
  print(i)
  data_plot2 <- data_plot %>%
    filter(datasource_weight =="GT" | 
             datasource_weight =="Infratest" |
             datasource_weight =="GT + election weight" |
             datasource_weight =="GT + weekly polls weight"
    ) %>%
    group_by(model_name) %>% 
    mutate(deviation_mean = mean(abs(Mean_dev), na.rm=TRUE)) %>%
    filter(election_date == i) %>%
    filter(model_time_interval_fac == "7 days" |
             model_time_interval_fac == "14 days" |
             model_time_interval_fac == "28 days" |
             model_time_interval_fac == "91 days") %>% 
    ungroup() %>%
    mutate(datasource_weight = recode(datasource_weight, 
                                      "GT" = "MC1: GT",
                                      "GT + election weight" = "MC2: GT + election weight",
                                      "GT + weekly polls weight" = "MC3: GT + weekly polls weight"))
  
  #  WHY NOT AGGREGATE DATASET?
  
  # Create x-axis tick labels
  x_tick_labels <- data_plot2 %>%
    group_by(GT_end_date) %>%
    filter(row_number()==1) %>%
    ungroup() %>%
    slice(c(1,25, 50, 75, 100, 125, 150)) %>%# Pick every 30th row
    select(GT_start_date, GT_end_date, model_time_distance)
  
  
  # Count number of models
  # data_plot2 %>% group_by(datasource_weight) %>% summarize(n_models = n())
  
  
  
  
  cols2 <- c("MC1: GT" = "#e41a1c", 
             #"Only polls" = "black", 
             "Infratest" = "black",
             "MC2: GT + election weight" = "#984ea3",
             "MC3: GT + weekly polls weight" = "#ff7f00")
  
  
  # Compare predictions across models
  # Table profide the number/percentage of models where MC1-3 beat polls
  
  model_comparison <- data_plot2 %>% 
    select(model_time_interval, model_time_distance, datasource_weight, deviation_mean) %>%
    group_by(datasource_weight) %>% 
    mutate(id = row_number()) %>%
    pivot_wider(names_from = datasource_weight, values_from =  deviation_mean) %>%
    group_by(model_time_interval, model_time_distance) %>%
    slice(1) %>% 
    ungroup() %>%
    mutate(id = row_number()) %>% # Recreate ID (aggregated)
    mutate(polls_vs_GT = ifelse(`Infratest` < `MC1: GT`, TRUE, FALSE),
           polls_vs_GTE = ifelse(`Infratest` < `MC2: GT + election weight`, TRUE, FALSE),
           polls_vs_GTP = ifelse(`Infratest` < `MC3: GT + weekly polls weight`, TRUE, FALSE))
  
  # TRUE = POLLS ARE BETTER, FALSE = MODELCLASS X is BETTER
  table(model_comparison$polls_vs_GT)
  prop.table(table(model_comparison$polls_vs_GT))
  table(model_comparison$polls_vs_GTE)
  prop.table(table(model_comparison$polls_vs_GTE))
  table(model_comparison$polls_vs_GTP)
  prop.table(table(model_comparison$polls_vs_GTP))
  
  # Comparison only for large width
  # TRUE = POLLS ARE BETTER, FALSE = MODELCLASS X is BETTER
  model_comparison_large_width <- model_comparison %>% 
    filter(model_time_interval == "7862400s (~13 weeks)")
  table(model_comparison_large_width$polls_vs_GT)
  prop.table(table(model_comparison_large_width$polls_vs_GT))
  table(model_comparison_large_width$polls_vs_GTE)
  prop.table(table(model_comparison_large_width$polls_vs_GTE))
  table(model_comparison_large_width$polls_vs_GTP)
  prop.table(table(model_comparison_large_width$polls_vs_GTP))
  
  
  # Add Plot X labels
  data_plot2 <- data_plot2 %>%
    mutate(model_time_interval_fac = recode(model_time_interval_fac,
                                            "7 days" = "Plot 1:\n7 days",
                                            "14 days" = "Plot 2:\n14 days",
                                            "28 days" = "Plot 3:\n28 days",
                                            "91 days" = "Plot 4:\n91 days"))
  
  
  
  
  #Plot GT vs. Polls
ggplot(data_plot2,
               aes(x = GT_end_date,
                   y = deviation_mean,
                   color = datasource_weight)) +
    geom_vline(xintercept = as.Date("2021-09-26"),
               linetype="dashed") +
    geom_vline(xintercept = as.Date("2017-09-24"),
               linetype="dashed") +
    geom_vline(xintercept = as.Date("2013-09-22"),
               linetype="dashed") +
    geom_vline(xintercept = as.Date("2009-09-27"),
               linetype="dashed") +
    geom_hline(yintercept = 0,
               linetype="solid") +  
    geom_point(size = 0.5) +
    geom_line() +
    theme_minimal(base_size = 22) +
    facet_grid(vars(model_time_interval_fac),
               #vars(election_date), 
               scales = "free_x") +
    scale_x_date(breaks = x_tick_labels$GT_end_date,
                 labels = paste0(x_tick_labels$model_time_distance, " day(s)\n[", x_tick_labels$GT_end_date, "]")
    ) +
    scale_y_continuous(sec.axis = dup_axis(
      name = "Width of data window")) +
    scale_color_manual(values = cols2) + 
    theme(axis.text.x = element_text(angle = 30, hjust = 1),
          legend.position="top",
          axis.text.y.right = element_blank(),
          plot.caption=element_text(hjust = 0),
          strip.background =element_rect(fill="white")) +
    labs(x = "Enddate of interval\n(= distance)",
         y = "MeanDeviation in %\n(prediction error)",
         colour = "Model class",
         caption = paste0("Note: Predictive models for ",i," election."),
         title = paste0("Election: ", i))
  #+ geom_errorbar(aes(ymin=dev_lower.ci, ymax=dev_upper.ci),width=.3, position=position_dodge(.9))

}





```


```{r}
#| label: fig-11
#| fig-cap: "Figure 6 with 'other parties'"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"


## Figure 6: Comparison elections ####
data_plot2 <- data_plot %>%
  filter(datasource_weight =="GT" | 
           datasource_weight =="Infratest" |
           datasource_weight =="GT + election weight" |
           datasource_weight =="GT + weekly polls weight"
  ) %>%
  group_by(model_name) %>% 
  mutate(deviation_mean = mean(abs(Mean_dev), na.rm=TRUE)) %>%
  #filter(election_date == i) %>%
  filter(model_time_interval_fac == "91 days") %>% 
  ungroup() %>%
  mutate(datasource_weight = recode(datasource_weight, 
                                    "GT" = "MC1: GT",
                                    "GT + election weight" = "MC2: GT + election weight",
                                    "GT + weekly polls weight" = "MC3: GT + weekly polls weight")) %>%
  mutate(election = factor(election, levels = c("18 Sep, 2005", 
                                                "27 Sep, 2009", "22 Sep, 2013", "24 Sep, 2017", "26 Sep, 2021"),
                           ordered = TRUE)) %>%
  mutate(election = recode(election, 
                           "18 Sep, 2005" = "18 Sep, 2005", 
                           "27 Sep, 2009" = "Plot 1:\nElection\n27 Sep, 2009", 
                           "22 Sep, 2013" = "Plot 2:\nElection\n22 Sep, 2013", 
                           "24 Sep, 2017" = "Plot 3:\nElection\n24 Sep, 2017", 
                           "26 Sep, 2021" = "Plot 4:\nElection\n26 Sep, 2021"))


#  WHY NOT AGGREGATE DATASET?

# Create x-axis tick labels
x_tick_labels <- data_plot2 %>%
  group_by(GT_end_date) %>%
  filter(row_number()==1) %>%
  ungroup() %>%
  slice(c(1,50, 100, 150)) %>%# Pick every 30th row
  select(GT_start_date, GT_end_date, model_time_distance)



# Compare predictions across models
# Table profide the number/percentage of models where MC1-3 beat polls

model_comparison <- data_plot2 %>% 
  select(model_time_interval, model_time_distance, election, datasource_weight, deviation_mean) %>%
  group_by(datasource_weight, election) %>% 
  mutate(id = row_number()) %>%
  pivot_wider(names_from = datasource_weight, values_from =  deviation_mean) %>%
  group_by(model_time_interval, model_time_distance, election) %>%
  slice(1) %>% 
  ungroup() %>%
  group_by(election) %>%
  arrange(election, model_time_interval, model_time_distance) %>%
  mutate(id = row_number()) %>% # Recreate ID (aggregated)
  ungroup() %>%
  mutate(polls_vs_GT = ifelse(`Infratest` < `MC1: GT`, TRUE, FALSE),
         polls_vs_GTE = ifelse(`Infratest` < `MC2: GT + election weight`, TRUE, FALSE),
         polls_vs_GTP = ifelse(`Infratest` < `MC3: GT + weekly polls weight`, TRUE, FALSE))

# TRUE = POLLS ARE BETTER, FALSE = MODELCLASS X is BETTER
    # table(model_comparison$election, model_comparison$polls_vs_GT)
    # prop.table(table(model_comparison$election, model_comparison$polls_vs_GT), margin = 1)
    # table(model_comparison$election, model_comparison$polls_vs_GTE)
    # prop.table(table(model_comparison$election, model_comparison$polls_vs_GTE), margin = 1)
    # table(model_comparison$election, model_comparison$polls_vs_GTP)
    # prop.table(table(model_comparison$election, model_comparison$polls_vs_GTP), margin = 1)



cols2 <- c("MC1: GT" = "#e41a1c", 
           #"Only polls" = "black", 
           "Infratest" = "black",
           "MC2: GT + election weight" = "#984ea3",
           "MC3: GT + weekly polls weight" = "#ff7f00")

add_election_word <- function(x){paste0("Election:\n",x)}

#Plot GT vs. Polls
ggplot(data_plot2,
             aes(x = GT_end_date,
                 y = deviation_mean,
                 color = datasource_weight)) +
  geom_vline(xintercept = as.Date("2021-09-26"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2017-09-24"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2013-09-22"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2009-09-27"),
             linetype="dashed") +
  geom_hline(yintercept = 0,
             linetype="solid") +  
  geom_point(size = 0.5) +
  geom_line() +
  theme_minimal(base_size = 22) +
  facet_grid2(rows = vars(election), 
              scales = "free",
              independent = "x") + # labeller = labeller(election = add_election_word)
  scale_y_continuous(sec.axis = dup_axis(
    name = "Election")) +
  scale_color_manual(values = cols2) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position="top",
        axis.text.y.right = element_blank(),
        plot.caption=element_text(hjust = 0),
        strip.background =element_rect(fill="white")) +
  labs(x = "Enddate of interval\n(= distance)",
       y = "MeanDeviation in %\n(prediction error)",
       colour = "Model class",
       caption = paste0("Note: Predictive models across all four elections holding the data window width constant at 91 days."),
       title = paste0("Election: All elections"))


```




## Replicating analysis across more elections


```{r}
#| label: fig-12
#| fig-cap: "Comparison across elections"
#| fig-width: 14
#| fig-height: 14
#| fig-pos: "H"


data_plot2 <- data_plot %>%
  filter(datasource_weight =="GT" | 
           datasource_weight =="Infratest" |
           datasource_weight =="GT + election weight" |
           datasource_weight =="GT + weekly polls weight"
  ) %>%
  group_by(model_name) %>% 
  mutate(deviation_mean = mean(abs(Mean_dev), na.rm=TRUE)) %>%
  #filter(election_date == i) %>%
  filter(model_time_interval_fac == "7 days" |
           model_time_interval_fac == "14 days" |
           model_time_interval_fac == "28 days" |
           model_time_interval_fac == "91 days") %>% 
  ungroup() %>%
  mutate(datasource_weight = recode(datasource_weight, 
                                    "GT" = "MC1: GT",
                                    "GT + election weight" = "MC2: GT + election weight",
                                    "GT + weekly polls weight" = "MC3: GT + weekly polls weight"))

#  WHY NOT AGGREGATE DATASET?

# Create x-axis tick labels
x_tick_labels <- data_plot2 %>%
  group_by(GT_end_date) %>%
  filter(row_number()==1) %>%
  ungroup() %>%
  slice(c(1,50, 100, 150)) %>%# Pick every 30th row
  select(GT_start_date, GT_end_date, model_time_distance)



# Count number of models
# data_plot2 %>% group_by(datasource_weight) %>% summarize(n_models = n())




cols2 <- c("MC1: GT" = "#e41a1c", 
           #"Only polls" = "black", 
           "Infratest" = "black",
           "MC2: GT + election weight" = "#984ea3",
           "MC3: GT + weekly polls weight" = "#ff7f00")

#Plot GT vs. Polls
ggplot(data_plot2,
             aes(x = GT_end_date,
                 y = deviation_mean,
                 color = datasource_weight)) +
  geom_vline(xintercept = as.Date("2021-09-26"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2017-09-24"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2013-09-22"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2009-09-27"),
             linetype="dashed") +
  geom_hline(yintercept = 0,
             linetype="solid") +  
  #geom_point(size = 0.5) +
  geom_line() +
  theme_minimal(base_size = 22) +
  # scale_x_date(breaks = x_tick_labels$GT_end_date,
  #              labels = paste0(x_tick_labels$model_time_distance, 
  #                              " day(s)\n[", 
  #                              x_tick_labels$GT_end_date, "]")
  # ) +
  facet_grid(vars(model_time_interval_fac),
             vars(election_date), 
             scales = "free_x") +
  scale_y_continuous(sec.axis = dup_axis(
    name = "Width of data window")) +
  scale_color_manual(values = cols2) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position="top",
        axis.text.y.right = element_blank(),
        plot.caption=element_text(hjust = 0)) +
  labs(x = "Enddate of interval\n(= distance)",
       y = "MeanDeviation in %\n(prediction error)",
       colour = "Model class",
       caption = paste0("Note: Predictive models across all four elections."),
       title = paste0("Election: All elections"))


```


## Benchmarking GT data and our models against all other leading polling institutes

```{r}
#| label: fig-13
#| fig-cap: "Benchmarking against other polls (Elections: 2009, 2013)"
#| fig-width: 14
#| fig-height: 20
#| fig-pos: "H"




# predictions for different distances #
data_plot <- data_predictions_final_mean %>%
  filter(election_date=="2009-09-27"|election_date=="2013-09-22") %>% #can filter for better overview
  mutate(model_time_interval_fac = factor(as.numeric(model_time_interval, "days"))) %>% # convert to days
  mutate(model_time_interval_fac = paste("Interval: ", model_time_interval_fac, " days", sep="")) %>%
  mutate(model_time_distance = election_date - GT_end_date)


data_plot$model_time_interval_fac <- factor(data_plot$model_time_interval_fac,
                                            levels = c("Interval: 7 days", "Interval: 14 days", "Interval: 21 days", 
                                                       "Interval: 28 days", "Interval: 42 days", "Interval: 56 days", 
                                                       "Interval: 70 days", "Interval: 77 days", "Interval: 84 days", "Interval: 91 days"),
                                            ordered = TRUE)

x_breaks <- unique(data_plot$GT_end_date)[seq(1, length(unique(data_plot$GT_end_date)), 10)]
x_labels_distance <- unique(data_plot$model_time_distance)[seq(1, length(unique(data_plot$model_time_distance)), 10)]
x_labels_date <- unique(data_plot$GT_end_date)[seq(1, length(unique(data_plot$GT_end_date)), 10)]

cols <- c("SPD" = "red", "CDU" = "black", "AFD" = "blue", 
          "FDP" = "orange", "Linke" = "purple", "Grüne" = "green")






# PLOT #


#### Plot average Deviation all Models
data_plot3 <- data_plot %>%
  filter(datasource_weight =="GT" | datasource_weight == "GT + election weight" | datasource_weight =="GT + weekly polls weight" | datasource_weight =="Infratest" |
           datasource_weight =="Forsa" | datasource_weight =="Kantar" | datasource_weight == "FGW" | datasource_weight =="Allens"
  ) %>%
  group_by(model_name) %>% 
  mutate(deviation_mean = mean(abs(Mean_dev) , na.rm=TRUE))

cols3 <- c("GT" = "red", "GT + election weight" = "purple",  "GT + weekly polls weight" = "green", 
           "Infratest" = "blue", "Forsa" = "sienna1", "Kantar" = "yellow3", "FGW" = "violetred", "Allens" = "peachpuff")


ggplot(data_plot3,
             aes(x = GT_end_date,
                 y = deviation_mean,
                 color = datasource_weight)) +
  geom_vline(xintercept = as.Date("2021-09-26"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2017-09-24"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2013-09-22"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2009-09-27"),
             linetype="dashed") +
  geom_hline(yintercept = 0,
             linetype="solid") +  
  #geom_point(size = 0.5) +
  geom_line() +
  theme_minimal() +
  facet_grid(vars(model_time_interval_fac),
             vars(election_date), 
             scales = "free") +
  #facet_wrap(~model_time_interval_fac, ncol = 1) +
  # xlim(min(data_plot$GT_end_date) - 1, as.Date("2021-09-26")+1) +
  scale_x_date(breaks = x_breaks,
               labels = paste("Distance: ",  x_labels_distance, " day(s)\n",
                              "Date: ", x_labels_date
               )
  ) +
  scale_color_manual(labels = c("GT", "GT + election weight", "GT + weekly polls weight", "Infratest Dimap", "Forsa", "Kantar", "Forschungsgruppe Wahlen", "IfD Allensbach"),values = cols3) + 
    theme_minimal(base_size = 22) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("MeanDeviation in %\n(prediction error)") +
  xlab("Enddate of interval\n(= distance)") +
  labs(colour = "Datasource") + 
  ylim(0,10) + 
  theme(legend.position="top")
#+geom_errorbar(aes(ymin=dev_lower.ci, ymax=dev_upper.ci),width=.3, position=position_dodge(.9))






```


```{r}
#| label: fig-14
#| fig-cap: "Benchmarking against other polls (Elections: 2017, 2021)"
#| fig-width: 14
#| fig-height: 20
#| fig-pos: "H"




# predictions for different distances #
data_plot <- data_predictions_final_mean %>%
  filter(election_date=="2017-09-24"|election_date=="2021-09-26") %>% #can filter for better overview
  mutate(model_time_interval_fac = factor(as.numeric(model_time_interval, "days"))) %>% # convert to days
  mutate(model_time_interval_fac = paste("Interval: ", model_time_interval_fac, " days", sep="")) %>%
  mutate(model_time_distance = election_date - GT_end_date)


data_plot$model_time_interval_fac <- factor(data_plot$model_time_interval_fac,
                                            levels = c("Interval: 7 days", "Interval: 14 days", "Interval: 21 days", 
                                                       "Interval: 28 days", "Interval: 42 days", "Interval: 56 days", 
                                                       "Interval: 70 days", "Interval: 77 days", "Interval: 84 days", "Interval: 91 days"),
                                            ordered = TRUE)

x_breaks <- unique(data_plot$GT_end_date)[seq(1, length(unique(data_plot$GT_end_date)), 10)]
x_labels_distance <- unique(data_plot$model_time_distance)[seq(1, length(unique(data_plot$model_time_distance)), 10)]
x_labels_date <- unique(data_plot$GT_end_date)[seq(1, length(unique(data_plot$GT_end_date)), 10)]

cols <- c("SPD" = "red", "CDU" = "black", "AFD" = "blue", 
          "FDP" = "orange", "Linke" = "purple", "Grüne" = "green")






# PLOT #


#### Plot average Deviation all Models
data_plot3 <- data_plot %>%
  filter(datasource_weight =="GT" | datasource_weight == "GT + election weight" | datasource_weight =="GT + weekly polls weight" | datasource_weight =="Infratest" |
           datasource_weight =="Forsa" | datasource_weight =="Kantar" | datasource_weight == "FGW" | datasource_weight =="Allens"
  ) %>%
  group_by(model_name) %>% 
  mutate(deviation_mean = mean(abs(Mean_dev) , na.rm=TRUE))

cols3 <- c("GT" = "red", "GT + election weight" = "purple",  "GT + weekly polls weight" = "green", 
           "Infratest" = "blue", "Forsa" = "sienna1", "Kantar" = "yellow3", "FGW" = "violetred", "Allens" = "peachpuff")


ggplot(data_plot3,
             aes(x = GT_end_date,
                 y = deviation_mean,
                 color = datasource_weight)) +
  geom_vline(xintercept = as.Date("2021-09-26"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2017-09-24"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2013-09-22"),
             linetype="dashed") +
  geom_vline(xintercept = as.Date("2009-09-27"),
             linetype="dashed") +
  geom_hline(yintercept = 0,
             linetype="solid") +  
  #geom_point(size = 0.5) +
  geom_line() +
  theme_minimal() +
  facet_grid(vars(model_time_interval_fac),
             vars(election_date), 
             scales = "free") +
  #facet_wrap(~model_time_interval_fac, ncol = 1) +
  # xlim(min(data_plot$GT_end_date) - 1, as.Date("2021-09-26")+1) +
  scale_x_date(breaks = x_breaks,
               labels = paste("Distance: ",  x_labels_distance, " day(s)\n",
                              "Date: ", x_labels_date
               )
  ) +
  scale_color_manual(labels = c("GT", "GT + election weight", "GT + weekly polls weight", "Infratest Dimap", "Forsa", "Kantar", "Forschungsgruppe Wahlen", "IfD Allensbach"),values = cols3) + 
    theme_minimal(base_size = 22) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("MeanDeviation in %\n(prediction error)") +
  xlab("Enddate of interval\n(= distance)") +
  labs(colour = "Datasource") + 
  ylim(0,10) + 
  theme(legend.position="top")
#+geom_errorbar(aes(ymin=dev_lower.ci, ymax=dev_upper.ci),width=.3, position=position_dodge(.9))






```


# Reproducing and replicating this study
One of our objectives with this study was to provide other researchers with a transparent workflow that involves all the steps from collecting the raw GT data, obtaining the predictions, to producing the analyses and the study itself. The replication files contain the following code files that correspond to the steps we followed to conduct this study:

* Step 1: Collect polling data (`1_Step_1_collect_polling_data.R`)
    + Contains the code to collect polling data from different polling companies in Germany including Infratest Dimap.
* Step 2: Collect GT data (`2_Step_2_collect_GT_data.R`)
    + Contains the code to collect the GT data.
* Step 3: Subsetting GT data (`3_Step_3_subset_GT_data.R`)
    + Contains the code to subset the GT data to obtain the samples on which we base our analysis.
* Step 4: Generate predictions (`4_Step_4_predictive_modelling.R`)
    + Contains the code to yield the predictions based on the GT data.
* Step 4: Conduct analyses and generate paper (`5_Step_5_generate_paper.qmd`)
    + Contains the code to analyse and visualize our predictions as well as generate the paper.




# References
